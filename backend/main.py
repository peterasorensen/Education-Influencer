"""
FastAPI Backend for Educational Video Generation Pipeline

Provides REST API and WebSocket endpoints for generating educational videos
with multi-voice narration, Manim animations, and audio synthesis.
"""

import logging
import asyncio
from pathlib import Path
from typing import Optional, Dict, Any, List
from datetime import datetime
from enum import Enum
import os
import uuid
import json

from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException, BackgroundTasks, UploadFile, File, Form
from fastapi.responses import FileResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import uvicorn
from dotenv import load_dotenv

from pipeline import (
    ScriptGenerator,
    AudioGenerator,
    TimestampExtractor,
    VisualScriptGenerator,  # Legacy - backward compatibility
    StoryboardGenerator,  # NEW
    LayoutEngine,  # NEW
    ManimGenerator,
    RemotionGenerator,  # NEW - Alternative to Manim
    ImageToVideoGenerator,
    LipsyncGenerator,
    VideoStitcher,
    ResumeDetector,
    CelebrityLoader,  # NEW - Dynamic celebrity loading
)

# Media upload modules
from pipeline.media_validator import MediaValidator
from pipeline.media_processor import MediaProcessor
from pipeline.media_storage import MediaStorage
from models.media_models import PhotoMetadata, AudioMetadata

# Load environment variables from .env file
load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# Initialize FastAPI app
app = FastAPI(
    title="Educational Video Generation API",
    description="Generate educational videos with AI-powered narration and Manim animations",
    version="1.0.0",
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:5173",
        "https://education-influencer.vercel.app",
        "https://eduvideo.bankrupt.fyi",
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Load environment variables
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    logger.warning("OPENAI_API_KEY not set. API calls will fail.")

REPLICATE_API_TOKEN = os.getenv("REPLICATE_API_TOKEN")
if not REPLICATE_API_TOKEN:
    logger.warning("REPLICATE_API_TOKEN not set. Celebrity video generation will fail.")

AUDIO_MODEL = os.getenv("AUDIO_MODEL")
if AUDIO_MODEL:
    logger.info(f"Using custom audio model: {AUDIO_MODEL}")
else:
    logger.info("Using default OpenAI TTS for audio generation")

# Output directories
BASE_OUTPUT_DIR = Path("./output")
BASE_OUTPUT_DIR.mkdir(exist_ok=True)

# Celebrity assets - Dynamic loading
ASSETS_DIR = Path("./assets")
celebrity_loader = CelebrityLoader(ASSETS_DIR)
CELEBRITY_IMAGES = celebrity_loader.get_celebrity_images()
CELEBRITY_AUDIO_SAMPLES = celebrity_loader.get_celebrity_audio()
ALL_CELEBRITIES = celebrity_loader.get_all_celebrities()

logger.info(f"Dynamically loaded {len(ALL_CELEBRITIES)} celebrities: {list(ALL_CELEBRITIES.keys())}")

# Initialize media upload modules
media_validator = MediaValidator()
media_processor = MediaProcessor()
media_storage = MediaStorage()


def cleanup_old_outputs(skip_cleanup: bool = False):
    """Clean up old output directories and media folder on startup to save space."""
    if skip_cleanup:
        logger.info("Skipping cleanup (resume mode enabled)")
        return

    import shutil

    # Clean up output directories
    if BASE_OUTPUT_DIR.exists():
        logger.info("Cleaning up old output directories...")
        cleaned = 0
        for old_dir in BASE_OUTPUT_DIR.iterdir():
            if old_dir.is_dir() and old_dir.exists():
                try:
                    shutil.rmtree(old_dir)
                    cleaned += 1
                except Exception as e:
                    logger.warning(f"Failed to clean up {old_dir}: {e}")
        logger.info(f"Cleaned up {cleaned} old output directories")

    # Clean up media directory (Manim autogenerated)
    media_dir = Path("./media")
    if media_dir.exists():
        logger.info("Cleaning up media directory...")
        try:
            shutil.rmtree(media_dir)
            logger.info("Media directory cleaned up")
        except Exception as e:
            logger.warning(f"Failed to clean up media directory: {e}")

# Active WebSocket connections
active_connections: Dict[str, WebSocket] = {}

# Job status tracking
job_status: Dict[str, Dict[str, Any]] = {}


# Request/Response Models
class VideoGenerationRequest(BaseModel):
    """Request model for video generation."""

    topic: str = Field(..., description="Educational topic for the video")
    duration_seconds: int = Field(
        default=60, ge=30, le=300, description="Target video duration in seconds"
    )
    quality: str = Field(
        default="medium_quality",
        description="Video quality (low_quality, medium_quality, high_quality)",
    )
    enable_subtitles: bool = Field(
        default=True, description="Whether to add subtitles to the final video"
    )
    celebrity: str = Field(
        default="drake",
        description="Celebrity for lip-synced video (LEGACY - use celebrities list instead)",
    )
    renderer: str = Field(
        default="manim",
        description="Animation renderer to use (manim, remotion)",
    )
    script_model: str = Field(
        default="gpt-4o",
        description="Model for script generation (gpt-4o, gpt-4o-mini, gpt-3.5-turbo)",
    )
    audio_model: str = Field(
        default="openai-tts",
        description="Model for audio generation (openai-tts, tortoise-tts, minimax-voice-cloning)",
    )
    lipsync_model: str = Field(
        default="tmappdev",
        description="Model for lip sync (tmappdev, kling, pixverse)",
    )
    video_model: str = Field(
        default="seedance",
        description="Model for image-to-video generation (seedance, kling-turbo)",
    )
    resume_job_id: Optional[str] = Field(
        default=None,
        description="Optional job ID to resume from (skips cleanup and completed steps)",
    )
    refined_context: Optional[Dict[str, Any]] = Field(
        default=None,
        description="Enhanced context from follow-up questions"
    )

    # NEW: Support multiple celebrities
    celebrities: Optional[List[Dict[str, Any]]] = Field(
        default=None,
        description="List of celebrity configurations (min 2). Each dict should have: mode, name (for preset), photo_id/audio_id (for custom), user_id"
    )

    # LEGACY: Single celebrity mode (backward compatibility)
    celebrity_mode: str = Field(
        default="preset",
        description="Celebrity mode: 'preset' (drake/sydney_sweeney) or 'custom' (user uploads) - LEGACY"
    )
    custom_photo_id: Optional[str] = Field(
        default=None,
        description="Photo ID for custom celebrity (required if celebrity_mode='custom') - LEGACY"
    )
    custom_audio_id: Optional[str] = Field(
        default=None,
        description="Audio ID for custom voice (required if celebrity_mode='custom') - LEGACY"
    )
    user_id: str = Field(
        default="default",
        description="User ID for accessing custom media"
    )


class QuestionType(str, Enum):
    MULTIPLE_CHOICE = "multiple_choice"
    MULTI_SELECT = "multi_select"
    SHORT_TEXT = "short_text"
    TOGGLE = "toggle"
    SLIDER = "slider"


class FollowUpQuestion(BaseModel):
    id: str
    question_text: str
    question_type: QuestionType
    category: str
    options: Optional[List[str]] = None
    default_value: Optional[Any] = None
    min_value: Optional[int] = None
    max_value: Optional[int] = None
    is_required: bool = False


class QuestionGenerationRequest(BaseModel):
    topic: str
    max_questions: int = Field(default=3, ge=2, le=4)


class QuestionGenerationResponse(BaseModel):
    questions: List[FollowUpQuestion]
    estimated_time_seconds: int


class PromptRefinementRequest(BaseModel):
    original_topic: str
    questions: List[FollowUpQuestion]
    answers: Dict[str, Any]


class PromptRefinementResponse(BaseModel):
    refined_prompt: str
    context: Dict[str, Any]


class VideoGenerationResponse(BaseModel):
    """Response model for video generation."""

    job_id: str = Field(..., alias="jobId", description="Unique job identifier")
    status: str = Field(..., description="Job status")
    message: str = Field(..., description="Status message")

    class Config:
        populate_by_name = True
        by_alias = True


class JobStatusResponse(BaseModel):
    """Response model for job status."""

    job_id: str
    status: str
    progress: int
    message: str
    video_url: Optional[str] = None
    error: Optional[str] = None


# Helper Functions
def get_pipeline_step_from_progress(progress: int) -> str:
    """Map progress percentage to pipeline step."""
    if progress < 15:
        return "generating_script"
    elif progress < 35:
        return "creating_audio"
    elif progress <= 45:
        return "extracting_timestamps"
    elif progress < 48:
        return "generating_storyboard"  # NEW step
    elif progress < 50:
        return "planning_visuals"  # Legacy / Layout step
    elif progress < 60:
        return "generating_animations"
    elif progress < 75:
        return "creating_celebrity_videos"
    elif progress < 90:
        return "lip_syncing"
    else:
        return "compositing_video"


def get_step_status(progress: int, step: str) -> str:
    """Determine status of a step based on current progress."""
    current_step = get_pipeline_step_from_progress(progress)
    steps_order = [
        "generating_script",
        "creating_audio",
        "extracting_timestamps",
        "generating_storyboard",  # NEW
        "planning_visuals",  # Legacy / Layout
        "generating_animations",
        "creating_celebrity_videos",
        "lip_syncing",
        "compositing_video"
    ]

    if steps_order.index(step) < steps_order.index(current_step):
        return "completed"
    elif step == current_step:
        return "in_progress"
    else:
        return "pending"


def calculate_step_progress(overall_progress: int, step: str) -> int:
    """Calculate individual step progress (0-100) based on overall progress."""
    step_ranges = {
        "generating_script": (0, 15),
        "creating_audio": (15, 35),
        "extracting_timestamps": (35, 45),
        "generating_storyboard": (45, 48),  # NEW
        "planning_visuals": (48, 50),  # Legacy / Layout
        "generating_animations": (50, 60),
        "creating_celebrity_videos": (60, 75),
        "lip_syncing": (75, 90),
        "compositing_video": (90, 100)
    }

    if step not in step_ranges:
        return 0

    start, end = step_ranges[step]

    if overall_progress < start:
        return 0
    elif overall_progress >= end:
        return 100
    else:
        # Calculate progress within this step's range
        step_range = end - start
        progress_in_step = overall_progress - start
        return int((progress_in_step / step_range) * 100)


async def send_progress_update(job_id: str, message: str, progress: int):
    """
    Send progress update via WebSocket.

    Args:
        job_id: Job identifier
        message: Progress message
        progress: Progress percentage (0-100)
    """
    # Update job status
    if job_id in job_status:
        job_status[job_id]["progress"] = progress
        job_status[job_id]["message"] = message
        job_status[job_id]["status"] = "processing"

    # Send WebSocket update if connected
    if job_id in active_connections:
        try:
            steps_order = [
                "generating_script",
                "creating_audio",
                "extracting_timestamps",
                "generating_storyboard",  # NEW
                "planning_visuals",  # Legacy / Layout
                "generating_animations",
                "creating_celebrity_videos",
                "lip_syncing",
                "compositing_video"
            ]

            # Send progress for ALL steps with their individual progress
            for step in steps_order:
                step_progress = calculate_step_progress(progress, step)
                current_step = get_pipeline_step_from_progress(progress)

                if step_progress == 100:
                    status = "completed"
                elif step == current_step:
                    status = "in_progress"
                else:
                    status = "pending"

                await active_connections[job_id].send_json({
                    "type": "progress",
                    "data": {
                        "step": step,
                        "status": status,
                        "message": message if step == current_step else "",
                        "progress": step_progress
                    }
                })
        except Exception as e:
            logger.error(f"Failed to send WebSocket update: {e}")


async def send_completion(job_id: str, video_url: str, topic: str = "", duration: float = 0):
    """Send completion notification via WebSocket."""
    if job_id in job_status:
        job_status[job_id]["status"] = "completed"
        job_status[job_id]["progress"] = 100
        job_status[job_id]["video_url"] = video_url

    if job_id in active_connections:
        try:
            # Mark all steps as completed
            steps_order = [
                "generating_script",
                "creating_audio",
                "extracting_timestamps",
                "generating_storyboard",  # NEW
                "planning_visuals",  # Legacy / Layout
                "generating_animations",
                "creating_celebrity_videos",
                "lip_syncing",
                "compositing_video"
            ]

            for step in steps_order:
                await active_connections[job_id].send_json({
                    "type": "progress",
                    "data": {
                        "step": step,
                        "status": "completed",
                        "message": f"{step.replace('_', ' ').title()} complete",
                        "progress": 100
                    }
                })

            # Send final completion message
            await active_connections[job_id].send_json({
                "type": "complete",
                "data": {
                    "videoUrl": video_url,
                    "duration": duration,
                    "topic": topic
                }
            })
        except Exception as e:
            logger.error(f"Failed to send completion: {e}")


async def send_error(job_id: str, error: str):
    """Send error notification via WebSocket."""
    if job_id in job_status:
        job_status[job_id]["status"] = "failed"
        job_status[job_id]["error"] = error

    if job_id in active_connections:
        try:
            await active_connections[job_id].send_json(
                {"type": "error", "message": error}
            )
        except Exception as e:
            logger.error(f"Failed to send error: {e}")


async def generate_video_pipeline(
    job_id: str,
    topic: str,
    duration_seconds: int,
    quality: str,
    enable_subtitles: bool,
    celebrity: str = "drake",
    renderer: str = "manim",
    resume_from_job: Optional[str] = None,
    refined_context: Optional[Dict[str, Any]] = None,
    celebrity_mode: str = "preset",
    custom_photo_id: Optional[str] = None,
    custom_audio_id: Optional[str] = None,
    user_id: str = "default",
    celebrities: Optional[List[Dict[str, Any]]] = None,
):
    """
    Main video generation pipeline with celebrity lip-sync.

    Args:
        job_id: Unique job identifier
        topic: Educational topic
        duration_seconds: Target duration
        quality: Video quality setting
        enable_subtitles: Whether to add subtitles
        celebrity: Celebrity for lip-synced video (LEGACY - used for backward compatibility)
        renderer: Animation renderer (manim, remotion)
        resume_from_job: Optional job ID to resume from (skips cleanup and completed steps)
        refined_context: Optional enhanced context from follow-up questions
        celebrity_mode: LEGACY - 'preset' or 'custom' (for backward compatibility)
        custom_photo_id: LEGACY - Photo ID for custom celebrity
        custom_audio_id: LEGACY - Audio ID for custom voice
        user_id: User ID for accessing custom media
        celebrities: NEW - List of celebrity configurations (supports 2+ celebrities)
                     Each dict: {mode, name, photo_id, audio_id, user_id}
    """
    try:
        # Wait for WebSocket connection before starting
        logger.info(f"Waiting for WebSocket connection for job {job_id}")
        max_wait_time = 30  # Wait up to 30 seconds for WebSocket connection
        wait_interval = 0.5
        waited = 0

        while job_id not in active_connections and waited < max_wait_time:
            await asyncio.sleep(wait_interval)
            waited += wait_interval

        if job_id not in active_connections:
            logger.warning(f"WebSocket not connected after {max_wait_time}s, proceeding anyway for job {job_id}")
        else:
            logger.info(f"WebSocket connected, starting video generation for job {job_id}: {topic}")

        # Ensure output directory exists
        BASE_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

        # Handle resume logic
        resume_mode = resume_from_job is not None
        if resume_mode:
            # Resume from existing job directory
            job_dir = BASE_OUTPUT_DIR / resume_from_job
            if not job_dir.exists():
                raise Exception(f"Cannot resume: job directory not found: {resume_from_job}")

            logger.info(f"RESUME MODE: Resuming from job {resume_from_job}")

            # Detect completed steps
            resume_detector = ResumeDetector(job_dir)
            completed_steps = resume_detector.detect_completed_steps()
            resume_point = resume_detector.get_resume_point()

            logger.info(resume_detector.get_summary())
            logger.info(f"Resuming from: {resume_point}")

            # Send resume summary to user
            await send_progress_update(
                job_id,
                f"Resuming from {resume_point} (skipping {sum(completed_steps.values())} completed steps)",
                5
            )
        else:
            # Create new job output directory
            job_dir = BASE_OUTPUT_DIR / job_id
            job_dir.mkdir(parents=True, exist_ok=True)
            completed_steps = {}
            resume_point = "script"

        # Handle celebrity configuration (NEW: supports multiple celebrities)
        # Priority 1: Use new celebrities list if provided
        # Priority 2: Fall back to legacy single celebrity mode for backward compatibility
        if celebrities is None or len(celebrities) == 0:
            # BACKWARD COMPATIBILITY: Convert legacy single celebrity to new format
            logger.info("Using legacy single celebrity mode, converting to new format")
            if celebrity_mode == "custom" and custom_photo_id:
                celebrities = [
                    {
                        "mode": "custom",
                        "photo_id": custom_photo_id,
                        "audio_id": custom_audio_id,
                        "user_id": user_id
                    }
                ]
            else:
                # Default: Drake + Sydney Sweeney
                celebrities = [
                    {"mode": "preset", "name": "drake", "user_id": "default"},
                    {"mode": "preset", "name": "sydney_sweeney", "user_id": "default"}
                ]

        logger.info(f"Using {len(celebrities)} celebrities for video generation")
        for idx, celeb in enumerate(celebrities):
            mode = celeb.get("mode", "preset")
            if mode == "preset":
                logger.info(f"  Celebrity {idx}: Preset '{celeb.get('name', 'unknown')}'")
            else:
                logger.info(f"  Celebrity {idx}: Custom (photo={celeb.get('photo_id')}, audio={celeb.get('audio_id')})")

        # Build celebrity image and audio maps
        celebrity_images_map = {}
        celebrity_audio_map = {}

        for celeb_idx, celeb_config in enumerate(celebrities):
            celeb_key = f"celebrity_{celeb_idx}"

            if celeb_config.get("mode") == "preset":
                # Use preset celebrity
                preset_name = celeb_config.get("name", "drake")
                celebrity_images_map[celeb_key] = CELEBRITY_IMAGES.get(preset_name, CELEBRITY_IMAGES["drake"])
                celebrity_audio_map[celeb_key] = CELEBRITY_AUDIO_SAMPLES.get(preset_name)
                logger.info(f"Mapped {celeb_key} to preset '{preset_name}'")
            else:
                # Use custom upload
                celeb_user_id = celeb_config.get("user_id", user_id)
                photo_id = celeb_config.get("photo_id")
                audio_id = celeb_config.get("audio_id")

                if photo_id:
                    custom_photo_path = media_storage.get_photo_path(celeb_user_id, photo_id)
                    if custom_photo_path and custom_photo_path.exists():
                        celebrity_images_map[celeb_key] = custom_photo_path
                        logger.info(f"Mapped {celeb_key} to custom photo: {custom_photo_path}")
                    else:
                        logger.warning(f"Custom photo not found for {celeb_key}, using Drake as fallback")
                        celebrity_images_map[celeb_key] = CELEBRITY_IMAGES["drake"]

                    if audio_id:
                        custom_audio_path = media_storage.get_audio_path(celeb_user_id, audio_id)
                        if custom_audio_path and custom_audio_path.exists():
                            celebrity_audio_map[celeb_key] = custom_audio_path
                            logger.info(f"Mapped {celeb_key} to custom audio: {custom_audio_path}")

        # Initialize pipeline modules with user-selected models
        script_gen = ScriptGenerator(OPENAI_API_KEY, model=script_model)

        # Map audio_model string to actual model identifier
        audio_model_map = {
            "openai-tts": None,  # None means use OpenAI TTS (default)
            "tortoise-tts": "lucataco/tortoise-tts:latest",
            "minimax-voice-cloning": "minimax/voice-cloning"
        }
        actual_audio_model = audio_model_map.get(audio_model)

        audio_gen = AudioGenerator(
            api_key=OPENAI_API_KEY,
            replicate_token=REPLICATE_API_TOKEN,
            audio_model=actual_audio_model,
            celebrity_audio_samples=CELEBRITY_AUDIO_SAMPLES,
        )
        timestamp_ext = TimestampExtractor(OPENAI_API_KEY)
        visual_gen = VisualScriptGenerator(OPENAI_API_KEY)  # Legacy - for backward compatibility
        storyboard_gen = StoryboardGenerator(OPENAI_API_KEY)  # NEW
        layout_engine = LayoutEngine()  # NEW
        manim_gen = ManimGenerator(OPENAI_API_KEY)
        remotion_gen = RemotionGenerator(OPENAI_API_KEY)  # NEW - Alternative to Manim

        # Image-to-video with user-selected model
        video_model_map = {
            "seedance": "bytedance/seedance-1-pro-fast",
            "kling-turbo": "kwaivgi/kling-v2.5-turbo-pro"
        }
        actual_video_model = video_model_map.get(video_model)
        img_to_video_gen = ImageToVideoGenerator(REPLICATE_API_TOKEN, model=actual_video_model)

        # Lip-sync with user-selected model
        lipsync_gen = LipsyncGenerator(REPLICATE_API_TOKEN, model=lipsync_model)

        video_stitcher = VideoStitcher()

        # Step 1: Generate Script
        script_path = job_dir / "script.json"
        if completed_steps.get("script"):
            logger.info("⏩ Skipping script generation (already completed)")
            script = json.loads(script_path.read_text(encoding="utf-8"))
            await send_progress_update(job_id, "Script loaded from cache", 10)
        else:
            await send_progress_update(job_id, "Generating script...", 5)

            # Generate speaker names from celebrities
            # For now, we support 2 speakers (teacher and student)
            # Map first celebrity to teacher, second to student
            if len(celebrities) >= 2:
                # Get names from first 2 celebrities
                teacher_celeb = celebrities[0]
                student_celeb = celebrities[1]

                # Determine speaker names
                if teacher_celeb.get("mode") == "preset":
                    teacher_name = teacher_celeb.get("name", "Drake").replace("_", " ").title()
                else:
                    teacher_name = "Speaker 1"

                if student_celeb.get("mode") == "preset":
                    student_name = student_celeb.get("name", "Sydney Sweeney").replace("_", " ").title()
                else:
                    student_name = "Speaker 2"

                speaker_names = {
                    "teacher": teacher_name,
                    "student": student_name
                }
                logger.info(f"Generated speaker names: {speaker_names}")
            else:
                # Fallback for single celebrity (shouldn't happen, but just in case)
                speaker_names = {
                    "teacher": "Speaker 1",
                    "student": "Speaker 2"
                }

            script = await script_gen.generate_script(
                topic=topic,
                duration_seconds=duration_seconds,
                speaker_names=speaker_names,
                refined_context=refined_context,
                progress_callback=lambda msg, prog: asyncio.create_task(
                    send_progress_update(job_id, msg, prog)
                ),
            )
            logger.info(f"Script generated with {len(script)} segments")

            # Save script
            script_path.write_text(json.dumps(script, indent=2), encoding="utf-8")

        # Build speaker-to-celebrity mapping for audio generation
        # Extract unique speakers from script and map them to celebrities
        unique_speakers = list(dict.fromkeys(seg["speaker"] for seg in script))  # Preserve order
        speaker_to_celebrity = {}

        for speaker_idx, speaker_name in enumerate(unique_speakers):
            celeb_idx = speaker_idx % len(celebrities)
            celeb_key = f"celebrity_{celeb_idx}"
            speaker_to_celebrity[speaker_name] = celeb_key
            logger.info(f"Audio mapping: speaker '{speaker_name}' -> {celeb_key}")

        # Step 2: Generate Audio
        audio_dir = job_dir / "audio_segments"
        final_audio_path = job_dir / "narration.mp3"
        if completed_steps.get("audio"):
            logger.info("⏩ Skipping audio generation (already completed)")

            # Load speaker voice map for resume functionality
            voice_map_path = job_dir / "speaker_voice_map.json"
            if voice_map_path.exists():
                try:
                    audio_gen.speaker_voice_map = json.loads(voice_map_path.read_text())
                    logger.info(f"Loaded speaker voice map: {audio_gen.speaker_voice_map}")
                except Exception as e:
                    logger.warning(f"Failed to load speaker voice map: {e}")

            await send_progress_update(job_id, "Audio loaded from cache", 25)
        else:
            await send_progress_update(job_id, "Generating audio...", 20)
            await audio_gen.generate_full_audio(
                script=script,
                output_dir=audio_dir,
                final_output_path=final_audio_path,
                progress_callback=lambda msg, prog: asyncio.create_task(
                    send_progress_update(job_id, msg, prog)
                ),
                speaker_celebrity_map=speaker_to_celebrity,
                celebrity_audio_samples=celebrity_audio_map,
            )
            logger.info(f"Audio generated: {final_audio_path}")

        # Step 3: Extract Timestamps
        srt_path = job_dir / "subtitles.srt"
        if completed_steps.get("timestamps"):
            logger.info("⏩ Skipping timestamp extraction (already completed)")
            # Load timestamp data from SRT (we'll extract it)
            # For now, we need to regenerate this step's data since we don't save timestamp_data
            await send_progress_update(job_id, "Re-extracting timestamps (needed for alignment)...", 38)
            timestamp_data = await timestamp_ext.extract_timestamps(
                audio_path=final_audio_path,
                output_srt_path=srt_path,
                progress_callback=lambda msg, prog: asyncio.create_task(
                    send_progress_update(job_id, msg, prog)
                ),
            )
            aligned_script = await timestamp_ext.align_script_with_timestamps(
                script=script,
                timestamp_data=timestamp_data,
            )
            await send_progress_update(job_id, "Timestamps loaded from cache", 40)
        else:
            await send_progress_update(job_id, "Extracting timestamps...", 35)
            timestamp_data = await timestamp_ext.extract_timestamps(
                audio_path=final_audio_path,
                output_srt_path=srt_path,
                progress_callback=lambda msg, prog: asyncio.create_task(
                    send_progress_update(job_id, msg, prog)
                ),
            )
            logger.info(f"Timestamps extracted: {len(timestamp_data['segments'])} segments")

            # Align script with timestamps
            aligned_script = await timestamp_ext.align_script_with_timestamps(
                script=script,
                timestamp_data=timestamp_data,
                progress_callback=lambda msg, prog: asyncio.create_task(
                    send_progress_update(job_id, msg, prog)
                ),
            )

        # Step 4: Generate Storyboard (NEW) or Visual Instructions (Legacy)
        storyboard_path = job_dir / "storyboard.json"
        visual_path = job_dir / "visual_instructions.json"

        # Try to use new storyboard system, fallback to legacy
        if completed_steps.get("storyboard"):
            logger.info("⏩ Skipping storyboard generation (already completed)")
            storyboard = json.loads(storyboard_path.read_text(encoding="utf-8"))
            await send_progress_update(job_id, "Storyboard loaded from cache", 47)
            visual_instructions = None  # Using new system
        elif completed_steps.get("visual_instructions"):
            logger.info("⏩ Using legacy visual instructions (already completed)")
            visual_instructions = json.loads(visual_path.read_text(encoding="utf-8"))
            await send_progress_update(job_id, "Visual instructions loaded from cache (legacy)", 47)
            storyboard = None  # Using legacy system
        else:
            # Generate new storyboard
            await send_progress_update(job_id, "Generating storyboard with spatial tracking...", 45)
            storyboard = await storyboard_gen.generate_storyboard(
                script=script,
                topic=topic,
                aligned_timestamps=aligned_script,
                progress_callback=lambda msg, prog: asyncio.create_task(
                    send_progress_update(job_id, msg, prog)
                ),
            )
            logger.info(f"Storyboard generated: {len(storyboard.get('scenes', []))} scenes")

            # Save storyboard
            storyboard_path.write_text(
                json.dumps(storyboard, indent=2), encoding="utf-8"
            )
            visual_instructions = None  # Not using legacy system

        # Step 5: Generate Animation Code (Manim or Remotion)
        audio_duration = timestamp_data.get('duration', 60.0)
        animation_video = None  # Will hold the final 9:8 video from either renderer

        if renderer == "remotion":
            # ===== REMOTION FLOW =====
            remotion_project_dir = job_dir / "remotion_project"

            if completed_steps.get("remotion_code"):
                logger.info("⏩ Skipping Remotion code generation (already completed)")
                await send_progress_update(job_id, "Remotion code loaded from cache", 50)
            else:
                await send_progress_update(job_id, "Generating Remotion code...", 48)

                if storyboard is not None:
                    logger.info("Using storyboard for Remotion code generation")
                    await send_progress_update(job_id, "Converting storyboard to Remotion code...", 49)
                    visual_instructions = storyboard.get('scenes', [])
                else:
                    logger.info("Using legacy visual instructions for Remotion code generation")

                # Generate Remotion code
                await remotion_gen.generate_remotion_code(
                    visual_instructions=visual_instructions,
                    topic=topic,
                    output_dir=remotion_project_dir,
                    target_duration=audio_duration,
                    progress_callback=lambda msg, prog: asyncio.create_task(
                        send_progress_update(job_id, msg, prog)
                    ),
                    script=script,
                )
                logger.info(f"Remotion code generated: {remotion_project_dir}")

            # Step 6: Render Remotion Video (TOP HALF - 9:8 format)
            remotion_output_path = job_dir / "remotion_output.mp4"

            if completed_steps.get("remotion_render"):
                logger.info("⏩ Skipping Remotion render (already completed)")
                if remotion_output_path.exists():
                    animation_video = remotion_output_path
                    logger.info(f"Using existing Remotion video: {animation_video}")
                    await send_progress_update(job_id, "Remotion video loaded from cache", 58)
                else:
                    logger.warning("Remotion render marked complete but video not found, re-rendering...")
                    completed_steps["remotion_render"] = False

            if not completed_steps.get("remotion_render"):
                for render_attempt in range(3):
                    try:
                        await send_progress_update(job_id, f"Rendering Remotion animation (attempt {render_attempt + 1}/3)...", 55)
                        animation_video = await remotion_gen.render_remotion_video(
                            project_dir=remotion_project_dir,
                            output_path=remotion_output_path,
                            composition_id="EducationalScene",
                            progress_callback=lambda msg, prog: asyncio.create_task(
                                send_progress_update(job_id, msg, prog)
                            ),
                        )
                        logger.info(f"Remotion video rendered: {animation_video}")
                        break
                    except Exception as render_error:
                        logger.warning(f"Remotion render attempt {render_attempt + 1} failed: {render_error}")
                        if render_attempt < 2:
                            await send_progress_update(job_id, f"Render failed, regenerating code...", 52)
                            await remotion_gen.generate_remotion_code(
                                visual_instructions=visual_instructions,
                                topic=topic,
                                output_dir=remotion_project_dir,
                                target_duration=audio_duration,
                                progress_callback=lambda msg, prog: asyncio.create_task(
                                    send_progress_update(job_id, msg, prog)
                                ),
                            )
                            logger.info(f"Remotion code regenerated after render failure")
                        else:
                            raise

            if not animation_video:
                raise Exception("Failed to render Remotion video after 3 attempts")

        else:
            # ===== MANIM FLOW (DEFAULT) =====
            manim_file = job_dir / "animation.py"

            if completed_steps.get("manim_code"):
                logger.info("⏩ Skipping Manim code generation (already completed)")
                await send_progress_update(job_id, "Manim code loaded from cache", 50)
            else:
                await send_progress_update(job_id, "Generating Manim code...", 48)

                if storyboard is not None:
                    logger.info("Using LayoutEngine with storyboard for Manim code generation")
                    await send_progress_update(job_id, "Converting storyboard to Manim code with spatial layout...", 49)
                    visual_instructions = storyboard.get('scenes', [])

                    await manim_gen.generate_manim_code(
                        visual_instructions=visual_instructions,
                        topic=topic,
                        output_path=manim_file,
                        target_duration=audio_duration,
                        progress_callback=lambda msg, prog: asyncio.create_task(
                            send_progress_update(job_id, msg, prog)
                        ),
                    )
                else:
                    logger.info("Using legacy visual instructions for Manim code generation")
                    await manim_gen.generate_manim_code(
                        visual_instructions=visual_instructions,
                        topic=topic,
                        output_path=manim_file,
                        target_duration=audio_duration,
                        progress_callback=lambda msg, prog: asyncio.create_task(
                            send_progress_update(job_id, msg, prog)
                        ),
                    )

                logger.info(f"Manim code generated: {manim_file}")

            # Step 6: Render Manim Video (TOP HALF - 9:8 format)
            manim_output_dir = job_dir / "manim_output"

            if completed_steps.get("manim_render"):
                logger.info("⏩ Skipping Manim render (already completed)")
                manim_videos = list(manim_output_dir.rglob("manim_output.mp4"))
                if manim_videos:
                    animation_video = manim_videos[0]
                    logger.info(f"Using existing Manim video: {animation_video}")
                    await send_progress_update(job_id, "Manim video loaded from cache", 58)
                else:
                    logger.warning("Manim render marked complete but video not found, re-rendering...")
                    completed_steps["manim_render"] = False

            if not completed_steps.get("manim_render"):
                for render_attempt in range(3):
                    try:
                        await send_progress_update(job_id, f"Rendering Manim animation (attempt {render_attempt + 1}/3)...", 55)
                        animation_video = await manim_gen.render_manim_video(
                            manim_file=manim_file,
                            output_dir=manim_output_dir,
                            quality=quality,
                            aspect_ratio="9:8",
                            progress_callback=lambda msg, prog: asyncio.create_task(
                                send_progress_update(job_id, msg, prog)
                            ),
                        )
                        logger.info(f"Manim video rendered: {animation_video}")
                        break
                    except Exception as render_error:
                        logger.warning(f"Manim render attempt {render_attempt + 1} failed: {render_error}")
                        if render_attempt < 2:
                            await send_progress_update(job_id, f"Render failed, regenerating code...", 52)
                            manim_file = await manim_gen.generate_manim_code(
                                visual_instructions=visual_instructions,
                                topic=topic,
                                output_path=manim_file,
                                target_duration=audio_duration,
                                progress_callback=lambda msg, prog: asyncio.create_task(
                                    send_progress_update(job_id, msg, prog)
                                ),
                            )
                            logger.info(f"Manim code regenerated after render failure")
                        else:
                            raise

            if not animation_video:
                raise Exception("Failed to render Manim video after 3 attempts")

        # Step 7: Generate Celebrity Videos from Image (BOTTOM HALF) - PER SEGMENT
        # Get individual audio segment files
        audio_segment_files = sorted(list(audio_dir.glob("segment_*.mp3")))
        logger.info(f"Found {len(audio_segment_files)} audio segments to process")

        if not audio_segment_files:
            raise Exception("No audio segments found! Cannot generate celebrity videos.")

        celebrity_video_dir = job_dir / "celebrity_videos"
        celebrity_video_dir.mkdir(parents=True, exist_ok=True)
        total_segments = len(audio_segment_files)

        if completed_steps.get("celebrity_videos"):
            logger.info("⏩ Skipping celebrity video generation (already completed)")
            # Load existing trimmed celebrity videos (prefer trimmed versions)
            celebrity_video_segments = sorted(list(celebrity_video_dir.glob("segment_*_trimmed.mp4")))
            if not celebrity_video_segments:
                # Fallback to non-trimmed if trimmed don't exist (backwards compatibility)
                celebrity_video_segments = sorted(list(celebrity_video_dir.glob("segment_*.mp4")))
            logger.info(f"Using {len(celebrity_video_segments)} existing celebrity videos")
            await send_progress_update(job_id, "Celebrity videos loaded from cache", 70)
        else:
            await send_progress_update(job_id, "Generating celebrity videos per segment in parallel...", 60)

            # speaker_to_celebrity mapping already built earlier (before audio generation)
            # We reuse it here for celebrity video generation

            # Helper function for generating and trimming a single celebrity video segment
            # This enables parallelization via asyncio.gather()
            async def generate_and_trim_celebrity_video(idx: int, audio_segment_path: Path) -> Path:
                """
                Generate and trim a celebrity video segment for parallel execution.

                Args:
                    idx: Segment index
                    audio_segment_path: Path to audio segment file

                Returns:
                    Path to trimmed celebrity video segment
                """
                try:
                    # Update progress (async, non-blocking)
                    segment_progress = int(60 + (idx / total_segments) * 15)
                    await send_progress_update(
                        job_id,
                        f"Generating celebrity video {idx + 1}/{total_segments}...",
                        segment_progress
                    )

                    # Get duration of this audio segment using ffprobe
                    segment_duration = await video_stitcher._get_duration(audio_segment_path)
                    logger.info(f"Segment {idx}: {segment_duration:.4f}s")

                    # Generate celebrity video for this segment
                    celebrity_segment_path = celebrity_video_dir / f"segment_{idx:03d}.mp4"

                    # Get speaker from script.json (loaded earlier) instead of parsing filename
                    # This is the correct way since filenames may have spaces and complex speaker names
                    speaker = script[idx]["speaker"]
                    logger.info(f"Segment {idx}: Speaker from script = '{speaker}'")

                    # Map speaker to celebrity using the speaker_to_celebrity mapping
                    celeb_key = speaker_to_celebrity.get(speaker, "celebrity_0")
                    segment_celebrity_image = celebrity_images_map.get(celeb_key, CELEBRITY_IMAGES["drake"])
                    logger.info(f"Segment {idx}: Mapped speaker '{speaker}' -> {celeb_key} -> {segment_celebrity_image}")

                    # Use varied prompts for natural variety
                    prompts = [
                        "natural talking expression, engaging eye contact, friendly smile, slight head nod",
                        "expressive speaking, warm expression, subtle gestures, natural movement",
                        "animated talking, enthusiastic expression, gentle head tilt, engaging presence",
                        "conversational speaking, genuine smile, soft eye contact, relaxed posture",
                        "energetic narration, dynamic expression, natural hand gestures, confident delivery",
                    ]
                    prompt = prompts[idx % len(prompts)]

                    await img_to_video_gen.generate_video_from_image(
                        image_path=segment_celebrity_image,
                        duration=segment_duration,
                        prompt=prompt,
                        output_path=celebrity_segment_path,
                        aspect_ratio="9:16",
                    )

                    # CRITICAL: Trim video to EXACT audio duration to prevent sync drift
                    # The image-to-video models (Seedance/Kling) don't generate exact durations
                    trimmed_segment_path = celebrity_video_dir / f"segment_{idx:03d}_trimmed.mp4"
                    logger.info(f"Trimming celebrity video {idx} to exact duration: {segment_duration:.4f}s")

                    await video_stitcher.trim_video_to_duration(
                        video_path=celebrity_segment_path,
                        duration=segment_duration,
                        output_path=trimmed_segment_path,
                    )

                    # Verify trimmed video duration
                    trimmed_duration = await video_stitcher._get_duration(trimmed_segment_path)
                    logger.info(f"Trimmed video {idx} duration: {trimmed_duration:.4f}s (target: {segment_duration:.4f}s, diff: {abs(trimmed_duration - segment_duration):.4f}s)")

                    logger.info(f"Celebrity video segment {idx} trimmed and ready: {trimmed_segment_path}")
                    return trimmed_segment_path

                except Exception as e:
                    logger.error(f"Failed to generate celebrity video for segment {idx}: {e}")
                    raise

            # Create tasks for all segments (parallelization pattern from audio_generator.py)
            celebrity_video_tasks = []
            for idx, audio_segment_path in enumerate(audio_segment_files):
                task = generate_and_trim_celebrity_video(idx, audio_segment_path)
                celebrity_video_tasks.append(task)

            # Execute all celebrity video generation tasks in parallel with rate limiting
            # Max 5-8 concurrent Replicate API calls to avoid rate limits
            MAX_CONCURRENT_REPLICATE_CALLS = 6
            semaphore = asyncio.Semaphore(MAX_CONCURRENT_REPLICATE_CALLS)

            async def with_rate_limit(task):
                """Wrap task with semaphore for rate limiting."""
                async with semaphore:
                    return await task

            logger.info(f"Starting parallel celebrity video generation for {len(celebrity_video_tasks)} segments (max {MAX_CONCURRENT_REPLICATE_CALLS} concurrent)")

            try:
                # Use return_exceptions=True to handle errors gracefully
                celebrity_video_segments = await asyncio.gather(
                    *[with_rate_limit(task) for task in celebrity_video_tasks],
                    return_exceptions=True
                )

                # Check for any errors and re-raise the first one
                for idx, result in enumerate(celebrity_video_segments):
                    if isinstance(result, Exception):
                        logger.error(f"Segment {idx} failed: {result}")
                        raise result

                logger.info(f"Successfully generated {len(celebrity_video_segments)} celebrity videos in parallel")

            except Exception as e:
                logger.error(f"Error during parallel celebrity video generation: {e}")
                raise

        # Step 8: Lip-sync Each Celebrity Video Segment with Its Audio
        lipsynced_video_dir = job_dir / "lipsynced_videos"
        lipsynced_video_dir.mkdir(parents=True, exist_ok=True)

        # Check if final concatenated video exists (for resume validation)
        lipsynced_video = job_dir / "celebrity_lipsynced_full.mp4"
        need_concatenation = False

        if completed_steps.get("lipsynced_videos"):
            logger.info("⏩ Skipping lip-sync (already completed)")
            # Load existing lip-synced videos
            lipsynced_segments = sorted(list(lipsynced_video_dir.glob("lipsynced_*.mp4")))
            logger.info(f"Using {len(lipsynced_segments)} existing lip-synced videos")

            # Check if concatenated video exists
            if not lipsynced_video.exists():
                logger.warning("⚠️  Concatenated video missing, will re-concatenate segments")
                need_concatenation = True
            else:
                logger.info(f"✅ Concatenated video exists: {lipsynced_video}")

            await send_progress_update(job_id, "Lip-synced videos loaded from cache", 85)
        else:
            await send_progress_update(job_id, "Lip-syncing video segments with audio in parallel...", 75)

            # Helper function for lip-syncing and trimming a single segment
            # This enables parallelization via asyncio.gather()
            async def lipsync_and_trim_segment(idx: int, video_segment: Path, audio_segment: Path) -> Path:
                """
                Lip-sync and trim a video segment for parallel execution.

                Args:
                    idx: Segment index
                    video_segment: Path to celebrity video segment
                    audio_segment: Path to audio segment file

                Returns:
                    Path to trimmed lip-synced video segment
                """
                try:
                    # Update progress (async, non-blocking)
                    segment_progress = int(75 + (idx / total_segments) * 15)
                    await send_progress_update(
                        job_id,
                        f"Lip-syncing segment {idx + 1}/{total_segments}...",
                        segment_progress
                    )

                    # Lip-sync this segment
                    lipsynced_segment_path = lipsynced_video_dir / f"lipsynced_{idx:03d}.mp4"

                    await lipsync_gen.sync_audio_to_video(
                        video_path=video_segment,
                        audio_path=audio_segment,
                        output_path=lipsynced_segment_path,
                    )

                    # Verify lip-synced video duration matches audio
                    lipsynced_duration = await video_stitcher._get_duration(lipsynced_segment_path)
                    audio_duration_check = await video_stitcher._get_duration(audio_segment)
                    logger.info(f"Lip-synced segment {idx} duration: {lipsynced_duration:.4f}s (audio: {audio_duration_check:.4f}s)")

                    # ALWAYS trim to match audio exactly (NO TOLERANCE)
                    # Even tiny differences accumulate over many segments causing drift
                    logger.info(f"Trimming lip-synced segment {idx} to EXACT audio duration (difference: {abs(lipsynced_duration - audio_duration_check):.4f}s)")
                    trimmed_lipsynced_path = lipsynced_video_dir / f"lipsynced_{idx:03d}_trimmed.mp4"
                    await video_stitcher.trim_video_to_duration(
                        video_path=lipsynced_segment_path,
                        duration=audio_duration_check,
                        output_path=trimmed_lipsynced_path,
                    )
                    logger.info(f"Lip-synced segment {idx} trimmed to exact audio duration: {trimmed_lipsynced_path}")
                    return trimmed_lipsynced_path

                except Exception as e:
                    logger.error(f"Failed to lip-sync segment {idx}: {e}")
                    raise

            # Create tasks for all segments (parallelization pattern from audio_generator.py)
            lipsync_tasks = []
            for idx, (video_segment, audio_segment) in enumerate(zip(celebrity_video_segments, audio_segment_files)):
                task = lipsync_and_trim_segment(idx, video_segment, audio_segment)
                lipsync_tasks.append(task)

            # Execute all lip-sync tasks in parallel with rate limiting
            # Max 5-8 concurrent Replicate API calls to avoid rate limits
            MAX_CONCURRENT_LIPSYNC_CALLS = 6
            lipsync_semaphore = asyncio.Semaphore(MAX_CONCURRENT_LIPSYNC_CALLS)

            async def with_lipsync_rate_limit(task):
                """Wrap task with semaphore for rate limiting."""
                async with lipsync_semaphore:
                    return await task

            logger.info(f"Starting parallel lip-sync for {len(lipsync_tasks)} segments (max {MAX_CONCURRENT_LIPSYNC_CALLS} concurrent)")

            try:
                # Use return_exceptions=True to handle errors gracefully
                lipsynced_segments = await asyncio.gather(
                    *[with_lipsync_rate_limit(task) for task in lipsync_tasks],
                    return_exceptions=True
                )

                # Check for any errors and re-raise the first one
                for idx, result in enumerate(lipsynced_segments):
                    if isinstance(result, Exception):
                        logger.error(f"Lip-sync segment {idx} failed: {result}")
                        raise result

                logger.info(f"Successfully lip-synced {len(lipsynced_segments)} segments in parallel")

            except Exception as e:
                logger.error(f"Error during parallel lip-sync generation: {e}")
                raise

            # Mark that we need to concatenate (just generated segments)
            need_concatenation = True

        # Step 8b: Concatenate all lip-synced segments into one video
        # Run if: (1) just generated segments OR (2) segments exist but full video missing
        if need_concatenation:
            await send_progress_update(job_id, "Concatenating lip-synced segments...", 88)

            # Calculate expected total duration from audio segments
            expected_duration = 0.0
            for audio_segment in audio_segment_files:
                segment_dur = await video_stitcher._get_duration(audio_segment)
                expected_duration += segment_dur
            logger.info(f"Expected concatenated duration from audio segments: {expected_duration:.4f}s")

            await video_stitcher.concatenate_videos(
                video_paths=lipsynced_segments,
                output_path=lipsynced_video,
                progress_callback=lambda msg, prog: asyncio.create_task(
                    send_progress_update(job_id, msg, int(88 + prog * 0.02))
                ),
            )

            # Verify concatenated video duration matches expected duration
            concatenated_duration = await video_stitcher._get_duration(lipsynced_video)
            duration_diff = abs(concatenated_duration - expected_duration)
            logger.info(f"Concatenated video duration: {concatenated_duration:.4f}s (expected: {expected_duration:.4f}s, diff: {duration_diff:.4f}s)")

            if duration_diff > 0.01:  # More than 10ms difference
                logger.warning(f"DURATION MISMATCH: Concatenated video differs from expected by {duration_diff:.4f}s")

            logger.info(f"All lip-synced segments concatenated: {lipsynced_video}")
        else:
            logger.info("✅ Using existing concatenated lip-synced video")

        # Step 9: Composite Top (Manim) and Bottom (Celebrity) into 9:16 Video
        composite_video = job_dir / "composite_video.mp4"

        if completed_steps.get("composite"):
            logger.info("⏩ Skipping composite (already completed)")
            await send_progress_update(job_id, "Composite video loaded from cache", 92)
        else:
            # CRITICAL: Verify all input durations BEFORE compositing to catch sync issues
            await send_progress_update(job_id, "Verifying video durations before compositing...", 89)

            animation_duration = await video_stitcher._get_duration(animation_video)
            lipsynced_duration = await video_stitcher._get_duration(lipsynced_video)

            logger.info(f"PRE-COMPOSITE DURATION CHECK:")
            logger.info(f"  Animation video ({renderer}): {animation_duration:.4f}s")
            logger.info(f"  Lipsynced video (with audio baked in): {lipsynced_duration:.4f}s")
            logger.info(f"  NOTE: Using audio from lipsynced video, NOT re-adding full_audio.mp3")

            # Check for duration mismatches (tolerance of 0.1s)
            # Animation should match lipsynced video duration
            if abs(animation_duration - lipsynced_duration) > 0.1:
                logger.warning(f"Animation video is {abs(animation_duration - lipsynced_duration):.2f}s off from lipsynced video")
                logger.warning("Animation will be trimmed to match lipsynced video duration")
            else:
                logger.info("Animation and lipsynced video durations match - compositing should maintain sync")

            await send_progress_update(job_id, "Compositing educational and celebrity videos...", 90)

            await video_stitcher.composite_top_bottom_videos(
                top_video_path=animation_video,
                bottom_video_path=lipsynced_video,
                audio_path=None,  # Not used - audio comes from bottom video
                output_path=composite_video,
                progress_callback=lambda msg, prog: asyncio.create_task(
                    send_progress_update(job_id, msg, int(90 + prog * 0.05))
                ),
            )

            # Verify composite duration matches lipsynced video (which has the audio)
            composite_duration = await video_stitcher._get_duration(composite_video)
            composite_diff = abs(composite_duration - lipsynced_duration)
            logger.info(f"Composite video duration: {composite_duration:.4f}s (lipsynced video: {lipsynced_duration:.4f}s, diff: {composite_diff:.4f}s)")

            if composite_diff > 0.1:
                logger.error(f"CRITICAL: Composite video duration mismatch! Off by {composite_diff:.2f}s")
                logger.error("This will cause audio/video sync issues in the final video")
            else:
                logger.info(f"SUCCESS: Final video duration matches celebrity_lipsynced_full.mp4 exactly!")

            logger.info(f"Composite video created: {composite_video}")

        # Step 9b: Re-extract Timestamps from Lip-Synced Audio (CRITICAL FOR SUBTITLE SYNC)
        # The lip-sync process modifies audio timing, so we need to re-extract timestamps
        # from the final lip-synced audio to ensure subtitles align perfectly
        if enable_subtitles:
            await send_progress_update(job_id, "Re-extracting timestamps from lip-synced audio for subtitle sync...", 93)

            # Extract audio from the lip-synced video (which has the ACTUAL timing we need)
            lipsynced_audio_path = job_dir / "lipsynced_audio.mp3"

            logger.info(f"Extracting audio from lip-synced video for accurate subtitle timing...")
            await video_stitcher.extract_audio(
                video_path=lipsynced_video,
                output_path=lipsynced_audio_path,
            )

            # Re-extract timestamps from the lip-synced audio (overwrite old srt_path)
            final_srt_path = job_dir / "subtitles_final.srt"
            logger.info(f"Re-extracting timestamps from lip-synced audio: {lipsynced_audio_path}")

            final_timestamp_data = await timestamp_ext.extract_timestamps(
                audio_path=lipsynced_audio_path,
                output_srt_path=final_srt_path,
                progress_callback=lambda msg, prog: asyncio.create_task(
                    send_progress_update(job_id, msg, int(93 + prog * 0.02))
                ),
            )
            logger.info(f"Final timestamps extracted from lip-synced audio: {len(final_timestamp_data['segments'])} segments")
            logger.info(f"✅ Subtitles will now be perfectly synced with lip-synced video!")

            # Use the new SRT file for subtitles
            srt_path = final_srt_path

        # Step 10: Add Subtitles (if enabled)
        final_video = job_dir / "final_video.mp4"

        if completed_steps.get("final"):
            logger.info("⏩ Final video already exists")
            await send_progress_update(job_id, "Final video loaded from cache", 98)
        elif enable_subtitles:
            await send_progress_update(job_id, "Adding subtitles...", 95)
            await video_stitcher.add_subtitles(
                video_path=composite_video,
                srt_path=srt_path,
                output_path=final_video,
                progress_callback=lambda msg, prog: asyncio.create_task(
                    send_progress_update(job_id, msg, int(95 + prog * 0.05))
                ),
            )
        else:
            # No subtitles, just use composite as final
            final_video = composite_video

        logger.info(f"Final video ready: {final_video}")

        # Send completion
        video_url = f"/api/videos/{job_id}/{final_video.name}"
        video_duration = timestamp_data.get('duration', 0) if 'timestamp_data' in locals() else 0
        await send_completion(job_id, video_url, topic=topic, duration=video_duration)

        logger.info(f"Video generation complete for job {job_id}")

    except Exception as e:
        logger.error(f"Video generation failed for job {job_id}: {e}", exc_info=True)
        await send_error(job_id, str(e))


# Startup event
@app.on_event("startup")
async def startup_event():
    """Startup event - no cleanup here (cleanup happens when user clicks generate)."""
    logger.info("Server started - output folders preserved for resume functionality")


# API Endpoints
@app.get("/")
async def root():
    """Root endpoint."""
    return {
        "name": "Educational Video Generation API",
        "version": "1.0.0",
        "status": "running",
    }


@app.get("/health")
async def health_check():
    """Health check endpoint."""
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}


@app.post("/api/generate", response_model=VideoGenerationResponse)
async def generate_video(
    request: VideoGenerationRequest, background_tasks: BackgroundTasks
):
    """
    Generate an educational video.

    Args:
        request: Video generation parameters

    Returns:
        Job information including job_id for tracking progress
    """
    try:
        # Validate API key
        if not OPENAI_API_KEY:
            raise HTTPException(
                status_code=500, detail="OpenAI API key not configured"
            )

        # Clean up old outputs before starting new job (skip if resuming)
        if request.resume_job_id:
            job_id = request.resume_job_id
            logger.info(f"Resuming job: {job_id}")
        else:
            # Clean up before creating new job
            logger.info("Cleaning up old outputs before starting new job...")
            # cleanup_old_outputs(skip_cleanup=False)

            job_id = str(uuid.uuid4())
            logger.info(f"Created new job: {job_id}")

        # Initialize job status
        job_status[job_id] = {
            "job_id": job_id,
            "status": "waiting_for_connection",
            "progress": 0,
            "message": "Waiting for WebSocket connection..." if not request.resume_job_id else "Resuming from previous job...",
            "topic": request.topic,
            "created_at": datetime.now().isoformat(),
        }

        # Start background task
        background_tasks.add_task(
            generate_video_pipeline,
            job_id=job_id,
            topic=request.topic,
            duration_seconds=request.duration_seconds,
            quality=request.quality,
            enable_subtitles=request.enable_subtitles,
            celebrity=request.celebrity,
            renderer=request.renderer,
            resume_from_job=request.resume_job_id,
            refined_context=request.refined_context,
            celebrity_mode=request.celebrity_mode,
            custom_photo_id=request.custom_photo_id,
            custom_audio_id=request.custom_audio_id,
            user_id=request.user_id,
            celebrities=request.celebrities,
        )

        logger.info(f"Video generation job created: {job_id}")

        return VideoGenerationResponse(
            job_id=job_id,
            status="waiting_for_connection",
            message="Video generation job created. Connect to WebSocket to start processing.",
        )

    except Exception as e:
        logger.error(f"Failed to create video generation job: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/jobs/{job_id}", response_model=JobStatusResponse)
async def get_job_status(job_id: str):
    """
    Get status of a video generation job.

    Args:
        job_id: Job identifier

    Returns:
        Current job status
    """
    if job_id not in job_status:
        raise HTTPException(status_code=404, detail="Job not found")

    status = job_status[job_id]
    return JobStatusResponse(
        job_id=status["job_id"],
        status=status["status"],
        progress=status.get("progress", 0),
        message=status.get("message", ""),
        video_url=status.get("video_url"),
        error=status.get("error"),
    )


@app.get("/api/videos/{job_id}/{filename}")
async def get_video(job_id: str, filename: str):
    """
    Download generated video.

    Args:
        job_id: Job identifier
        filename: Video filename

    Returns:
        Video file
    """
    video_path = BASE_OUTPUT_DIR / job_id / filename

    if not video_path.exists():
        raise HTTPException(status_code=404, detail="Video not found")

    return FileResponse(
        path=video_path,
        media_type="video/mp4",
        filename=f"{job_id}_{filename}",
    )


@app.get("/api/celebrities")
async def get_celebrities():
    """
    Get list of all available celebrities with their metadata.

    Returns:
        List of celebrities with id, name, and image URL
    """
    celebrities_list = []
    for celeb_id, config in ALL_CELEBRITIES.items():
        celebrities_list.append({
            "id": celeb_id,
            "name": config["name"],
            "imageUrl": f"/api/celebrities/{celeb_id}/image",
            "hasAudio": config.get("audio") is not None,
        })

    return {"celebrities": celebrities_list}


@app.get("/api/celebrities/{celebrity_id}/image")
async def get_celebrity_image(celebrity_id: str):
    """
    Get celebrity image file.

    Args:
        celebrity_id: Celebrity identifier

    Returns:
        Image file
    """
    if celebrity_id not in ALL_CELEBRITIES:
        raise HTTPException(status_code=404, detail="Celebrity not found")

    image_path = ALL_CELEBRITIES[celebrity_id].get("image")
    if not image_path or not image_path.exists():
        raise HTTPException(status_code=404, detail="Celebrity image not found")

    # Determine media type from extension
    extension = image_path.suffix.lower()
    media_type_map = {
        ".png": "image/png",
        ".jpg": "image/jpeg",
        ".jpeg": "image/jpeg",
        ".webp": "image/webp",
        ".gif": "image/gif",
    }
    media_type = media_type_map.get(extension, "image/png")

    from fastapi.responses import Response

    # Read image file
    with open(image_path, 'rb') as f:
        image_data = f.read()

    # Return as Response with proper headers for inline display
    return Response(
        content=image_data,
        media_type=media_type,
        headers={
            'Cache-Control': 'public, max-age=3600',
        }
    )


@app.post("/api/generate-questions", response_model=QuestionGenerationResponse)
async def generate_follow_up_questions(request: QuestionGenerationRequest):
    """Generate 2-4 follow-up questions (<2s latency)."""
    try:
        from pipeline.followup_generator import FollowUpQuestionGenerator
        generator = FollowUpQuestionGenerator(OPENAI_API_KEY)
        questions = await generator.generate_questions(
            topic=request.topic,
            max_questions=request.max_questions,
            time_budget_seconds=2.0
        )

        # Convert to Pydantic models
        pydantic_questions = []
        for q in questions:
            pydantic_questions.append(FollowUpQuestion(
                id=q.id,
                question_text=q.question_text,
                question_type=QuestionType(q.question_type),
                category=q.category,
                options=q.options,
                default_value=q.default_value,
                min_value=q.min_value,
                max_value=q.max_value,
                is_required=q.is_required
            ))

        # Estimate time based on question types
        estimated_time = sum(
            15 if q.question_type == QuestionType.MULTIPLE_CHOICE else 20
            for q in pydantic_questions
        )

        return QuestionGenerationResponse(
            questions=pydantic_questions,
            estimated_time_seconds=estimated_time
        )
    except Exception as e:
        logger.error(f"Question generation failed: {e}")
        return QuestionGenerationResponse(questions=[], estimated_time_seconds=0)


@app.post("/api/refine-prompt", response_model=PromptRefinementResponse)
async def refine_prompt(request: PromptRefinementRequest):
    """Merge answers into refined prompt."""
    try:
        from pipeline.followup_generator import FollowUpQuestionGenerator, FollowUpQuestion as FUQ
        generator = FollowUpQuestionGenerator(OPENAI_API_KEY)

        # Convert Pydantic models back to internal FollowUpQuestion objects
        internal_questions = []
        for q in request.questions:
            internal_q = FUQ(
                id=q.id,
                question_text=q.question_text,
                question_type=q.question_type.value,
                category=q.category,
                options=q.options,
                default_value=q.default_value,
                min_value=q.min_value,
                max_value=q.max_value,
                is_required=q.is_required
            )
            internal_questions.append(internal_q)

        refined_prompt = await generator.merge_answers_into_prompt(
            original_topic=request.original_topic,
            questions=internal_questions,
            answers=request.answers
        )

        # Extract structured context
        context = {
            "original_topic": request.original_topic,
            "audience": None,
            "complexity_level": None,
            "focus_areas": [],
            "teaching_style": []
        }

        for question in request.questions:
            answer = request.answers.get(question.id)
            if not answer:
                continue

            if question.category == "audience":
                context["audience"] = answer
            elif question.category == "depth":
                context["complexity_level"] = answer
            elif question.category == "focus":
                if isinstance(answer, list):
                    context["focus_areas"].extend(answer)
                else:
                    context["focus_areas"].append(answer)
            elif question.category == "style":
                if isinstance(answer, list):
                    context["teaching_style"].extend(answer)
                else:
                    context["teaching_style"].append(answer)

        return PromptRefinementResponse(
            refined_prompt=refined_prompt,
            context=context
        )
    except Exception as e:
        logger.error(f"Prompt refinement failed: {e}")
        return PromptRefinementResponse(
            refined_prompt=request.original_topic,
            context={"original_topic": request.original_topic}
        )


@app.post("/api/upload/photo", response_model=PhotoMetadata)
async def upload_photo(photo: UploadFile = File(...), user_id: str = Form("default")):
    """
    Upload a custom photo for celebrity video.

    Args:
        photo: Photo file (JPEG, PNG, WebP)
        user_id: User identifier

    Returns:
        PhotoMetadata with URLs to access the photo
    """
    try:
        logger.info(f"Uploading photo for user {user_id}: {photo.filename}")

        # Validate photo
        photo_data, sanitized_filename = await media_validator.validate_photo(photo)

        # Process photo (resize, create thumbnail, strip EXIF)
        processed_photo, thumbnail, dimensions = media_processor.process_photo(photo_data)

        # Save to storage
        metadata = media_storage.save_photo(
            user_id=user_id,
            photo_data=processed_photo,
            thumbnail_data=thumbnail,
            filename=sanitized_filename,
            dimensions=dimensions
        )

        logger.info(f"Photo uploaded successfully: {metadata.photo_id}")
        return metadata

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Photo upload failed: {e}")
        raise HTTPException(status_code=500, detail=f"Photo upload failed: {str(e)}")


@app.post("/api/upload/audio", response_model=AudioMetadata)
async def upload_audio(audio: UploadFile = File(...), user_id: str = Form("default")):
    """
    Upload a custom audio clip for voice cloning.

    Args:
        audio: Audio file (MP3, WAV, WebM), 2-5 seconds
        user_id: User identifier

    Returns:
        AudioMetadata with URL to access the audio
    """
    try:
        logger.info(f"Uploading audio for user {user_id}: {audio.filename}")

        # Validate audio
        audio_data, sanitized_filename, duration = await media_validator.validate_audio(audio)

        # Process audio (convert to mono MP3 at 24kHz, trim if needed)
        processed_audio, final_duration, sample_rate = media_processor.process_audio(
            audio_data, sanitized_filename
        )

        # Save to storage
        metadata = media_storage.save_audio(
            user_id=user_id,
            audio_data=processed_audio,
            filename=sanitized_filename,
            duration=final_duration,
            sample_rate=sample_rate
        )

        logger.info(f"Audio uploaded successfully: {metadata.audio_id}")
        return metadata

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Audio upload failed: {e}")
        raise HTTPException(status_code=500, detail=f"Audio upload failed: {str(e)}")


@app.get("/api/media/user/{user_id}")
async def get_user_media(user_id: str):
    """
    Get all uploaded media for a user.

    Args:
        user_id: User identifier

    Returns:
        Dictionary with 'photos' and 'audio' lists
    """
    try:
        media = media_storage.get_user_media(user_id)
        return media
    except Exception as e:
        logger.error(f"Failed to get user media: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get user media: {str(e)}")


@app.get("/api/media/photos/{user_id}/{filename}")
async def get_photo(user_id: str, filename: str):
    """
    Get a photo file.

    Args:
        user_id: User identifier
        filename: Photo filename (e.g., photo_id.jpg or photo_id_thumb.jpg)

    Returns:
        Photo file
    """
    try:
        photo_dir = media_storage._get_user_photo_dir(user_id)
        photo_path = photo_dir / filename

        if not photo_path.exists():
            raise HTTPException(status_code=404, detail="Photo not found")

        return FileResponse(
            path=photo_path,
            media_type="image/jpeg",
            filename=filename
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get photo: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get photo: {str(e)}")


@app.get("/api/media/audio/{user_id}/{filename}")
async def get_audio(user_id: str, filename: str):
    """
    Get an audio file.

    Args:
        user_id: User identifier
        filename: Audio filename (e.g., audio_id.mp3)

    Returns:
        Audio file
    """
    try:
        audio_dir = media_storage._get_user_audio_dir(user_id)
        audio_path = audio_dir / filename

        if not audio_path.exists():
            raise HTTPException(status_code=404, detail="Audio not found")

        return FileResponse(
            path=audio_path,
            media_type="audio/mpeg",
            filename=filename
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get audio: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get audio: {str(e)}")


@app.delete("/api/media/photos/{user_id}/{photo_id}")
async def delete_photo(user_id: str, photo_id: str):
    """
    Delete a photo.

    Args:
        user_id: User identifier
        photo_id: Photo identifier

    Returns:
        Success message
    """
    try:
        success = media_storage.delete_photo(user_id, photo_id)
        if not success:
            raise HTTPException(status_code=404, detail="Photo not found")

        return {"message": f"Photo {photo_id} deleted successfully"}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to delete photo: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to delete photo: {str(e)}")


@app.delete("/api/media/audio/{user_id}/{audio_id}")
async def delete_audio(user_id: str, audio_id: str):
    """
    Delete an audio file.

    Args:
        user_id: User identifier
        audio_id: Audio identifier

    Returns:
        Success message
    """
    try:
        success = media_storage.delete_audio(user_id, audio_id)
        if not success:
            raise HTTPException(status_code=404, detail="Audio not found")

        return {"message": f"Audio {audio_id} deleted successfully"}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to delete audio: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to delete audio: {str(e)}")


@app.websocket("/ws/{job_id}")
async def websocket_endpoint(websocket: WebSocket, job_id: str):
    """
    WebSocket endpoint for real-time progress updates.

    Args:
        websocket: WebSocket connection
        job_id: Job identifier to track
    """
    await websocket.accept()
    active_connections[job_id] = websocket

    try:
        logger.info(f"WebSocket connected for job {job_id}")

        # Send current status if job exists
        if job_id in job_status:
            status = job_status[job_id]
            await websocket.send_json(
                {
                    "type": "status",
                    "status": status["status"],
                    "progress": status.get("progress", 0),
                    "message": status.get("message", ""),
                }
            )

        # Keep connection alive with periodic pings
        last_ping = asyncio.get_event_loop().time()
        while True:
            try:
                # Wait for messages with timeout
                await asyncio.wait_for(websocket.receive_text(), timeout=1.0)
            except asyncio.TimeoutError:
                # Send ping every 30 seconds to keep connection alive through Cloudflare
                current_time = asyncio.get_event_loop().time()
                if current_time - last_ping > 30:
                    try:
                        await websocket.send_json({"type": "ping"})
                        last_ping = current_time
                    except Exception as e:
                        logger.warning(f"Failed to send ping: {e}")
                        break
                continue
            except WebSocketDisconnect:
                break

    except WebSocketDisconnect:
        logger.info(f"WebSocket disconnected for job {job_id}")
    except Exception as e:
        logger.error(f"WebSocket error for job {job_id}: {e}")
    finally:
        if job_id in active_connections:
            del active_connections[job_id]


# Run application
if __name__ == "__main__":
    import sys

    port = int(sys.argv[1]) if len(sys.argv) > 1 else 8000

    logger.info(f"Starting Educational Video Generation API on port {port}")

    import os
    is_production = os.getenv("ENVIRONMENT") == "production"

    # Configure reload to ignore all directories where we write files during video generation
    # This prevents WebSocket disconnections when files are created
    if not is_production:
        logger.info("Development mode: Auto-reload DISABLED to prevent WebSocket disconnections during video generation")
        logger.info("Please manually restart the server when you make code changes")
        reload_enabled = False
    else:
        reload_enabled = False

    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=port,
        reload=reload_enabled,
        log_level="info",
    )
