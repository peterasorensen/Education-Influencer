"""
FastAPI Backend for Educational Video Generation Pipeline

Provides REST API and WebSocket endpoints for generating educational videos
with multi-voice narration, Manim animations, and audio synthesis.
"""

import logging
import asyncio
from pathlib import Path
from typing import Optional, Dict, Any
from datetime import datetime
import os
import uuid
import json

from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException, BackgroundTasks
from fastapi.responses import FileResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import uvicorn
from dotenv import load_dotenv

from pipeline import (
    ScriptGenerator,
    AudioGenerator,
    TimestampExtractor,
    VisualScriptGenerator,
    ManimGenerator,
    ImageToVideoGenerator,
    LipsyncGenerator,
    VideoStitcher,
    ResumeDetector,
)

# Load environment variables from .env file
load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# Initialize FastAPI app
app = FastAPI(
    title="Educational Video Generation API",
    description="Generate educational videos with AI-powered narration and Manim animations",
    version="1.0.0",
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:5173",
        "https://education-influencer.vercel.app",
        "https://eduvideo.bankrupt.fyi",
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Load environment variables
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    logger.warning("OPENAI_API_KEY not set. API calls will fail.")

REPLICATE_API_TOKEN = os.getenv("REPLICATE_API_TOKEN")
if not REPLICATE_API_TOKEN:
    logger.warning("REPLICATE_API_TOKEN not set. Celebrity video generation will fail.")

# Output directories
BASE_OUTPUT_DIR = Path("./output")
BASE_OUTPUT_DIR.mkdir(exist_ok=True)

# Celebrity assets
ASSETS_DIR = Path("./assets")
CELEBRITY_IMAGES = {
    "drake": ASSETS_DIR / "drake.jpg",
    "sydney_sweeney": ASSETS_DIR / "sydneysweeney.png",
}


def cleanup_old_outputs(skip_cleanup: bool = False):
    """Clean up old output directories and media folder on startup to save space."""
    if skip_cleanup:
        logger.info("Skipping cleanup (resume mode enabled)")
        return

    import shutil

    # Clean up output directories
    if BASE_OUTPUT_DIR.exists():
        logger.info("Cleaning up old output directories...")
        cleaned = 0
        for old_dir in BASE_OUTPUT_DIR.iterdir():
            if old_dir.is_dir() and old_dir.exists():
                try:
                    shutil.rmtree(old_dir)
                    cleaned += 1
                except Exception as e:
                    logger.warning(f"Failed to clean up {old_dir}: {e}")
        logger.info(f"Cleaned up {cleaned} old output directories")

    # Clean up media directory (Manim autogenerated)
    media_dir = Path("./media")
    if media_dir.exists():
        logger.info("Cleaning up media directory...")
        try:
            shutil.rmtree(media_dir)
            logger.info("Media directory cleaned up")
        except Exception as e:
            logger.warning(f"Failed to clean up media directory: {e}")

# Active WebSocket connections
active_connections: Dict[str, WebSocket] = {}

# Job status tracking
job_status: Dict[str, Dict[str, Any]] = {}


# Request/Response Models
class VideoGenerationRequest(BaseModel):
    """Request model for video generation."""

    topic: str = Field(..., description="Educational topic for the video")
    duration_seconds: int = Field(
        default=60, ge=30, le=300, description="Target video duration in seconds"
    )
    quality: str = Field(
        default="medium_quality",
        description="Video quality (low_quality, medium_quality, high_quality)",
    )
    enable_subtitles: bool = Field(
        default=True, description="Whether to add subtitles to the final video"
    )
    celebrity: str = Field(
        default="drake",
        description="Celebrity for lip-synced video (drake, sydney_sweeney)",
    )
    resume_job_id: Optional[str] = Field(
        default=None,
        description="Optional job ID to resume from (skips cleanup and completed steps)",
    )


class VideoGenerationResponse(BaseModel):
    """Response model for video generation."""

    job_id: str = Field(..., alias="jobId", description="Unique job identifier")
    status: str = Field(..., description="Job status")
    message: str = Field(..., description="Status message")

    class Config:
        populate_by_name = True
        by_alias = True


class JobStatusResponse(BaseModel):
    """Response model for job status."""

    job_id: str
    status: str
    progress: int
    message: str
    video_url: Optional[str] = None
    error: Optional[str] = None


# Helper Functions
def get_pipeline_step_from_progress(progress: int) -> str:
    """Map progress percentage to pipeline step."""
    if progress < 15:
        return "generating_script"
    elif progress < 35:
        return "creating_audio"
    elif progress <= 45:
        return "extracting_timestamps"
    elif progress < 50:
        return "planning_visuals"
    elif progress < 60:
        return "generating_animations"
    elif progress < 75:
        return "creating_celebrity_videos"
    elif progress < 90:
        return "lip_syncing"
    else:
        return "compositing_video"


def get_step_status(progress: int, step: str) -> str:
    """Determine status of a step based on current progress."""
    current_step = get_pipeline_step_from_progress(progress)
    steps_order = [
        "generating_script",
        "creating_audio",
        "extracting_timestamps",
        "planning_visuals",
        "generating_animations",
        "creating_celebrity_videos",
        "lip_syncing",
        "compositing_video"
    ]

    if steps_order.index(step) < steps_order.index(current_step):
        return "completed"
    elif step == current_step:
        return "in_progress"
    else:
        return "pending"


def calculate_step_progress(overall_progress: int, step: str) -> int:
    """Calculate individual step progress (0-100) based on overall progress."""
    step_ranges = {
        "generating_script": (0, 15),
        "creating_audio": (15, 35),
        "extracting_timestamps": (35, 45),
        "planning_visuals": (45, 50),
        "generating_animations": (50, 60),
        "creating_celebrity_videos": (60, 75),
        "lip_syncing": (75, 90),
        "compositing_video": (90, 100)
    }

    if step not in step_ranges:
        return 0

    start, end = step_ranges[step]

    if overall_progress < start:
        return 0
    elif overall_progress >= end:
        return 100
    else:
        # Calculate progress within this step's range
        step_range = end - start
        progress_in_step = overall_progress - start
        return int((progress_in_step / step_range) * 100)


async def send_progress_update(job_id: str, message: str, progress: int):
    """
    Send progress update via WebSocket.

    Args:
        job_id: Job identifier
        message: Progress message
        progress: Progress percentage (0-100)
    """
    # Update job status
    if job_id in job_status:
        job_status[job_id]["progress"] = progress
        job_status[job_id]["message"] = message
        job_status[job_id]["status"] = "processing"

    # Send WebSocket update if connected
    if job_id in active_connections:
        try:
            steps_order = [
                "generating_script",
                "creating_audio",
                "extracting_timestamps",
                "planning_visuals",
                "generating_animations",
                "creating_celebrity_videos",
                "lip_syncing",
                "compositing_video"
            ]

            # Send progress for ALL steps with their individual progress
            for step in steps_order:
                step_progress = calculate_step_progress(progress, step)
                current_step = get_pipeline_step_from_progress(progress)

                if step_progress == 100:
                    status = "completed"
                elif step == current_step:
                    status = "in_progress"
                else:
                    status = "pending"

                await active_connections[job_id].send_json({
                    "type": "progress",
                    "data": {
                        "step": step,
                        "status": status,
                        "message": message if step == current_step else "",
                        "progress": step_progress
                    }
                })
        except Exception as e:
            logger.error(f"Failed to send WebSocket update: {e}")


async def send_completion(job_id: str, video_url: str, topic: str = "", duration: float = 0):
    """Send completion notification via WebSocket."""
    if job_id in job_status:
        job_status[job_id]["status"] = "completed"
        job_status[job_id]["progress"] = 100
        job_status[job_id]["video_url"] = video_url

    if job_id in active_connections:
        try:
            # Mark all steps as completed
            steps_order = [
                "generating_script",
                "creating_audio",
                "extracting_timestamps",
                "planning_visuals",
                "generating_animations",
                "creating_celebrity_videos",
                "lip_syncing",
                "compositing_video"
            ]

            for step in steps_order:
                await active_connections[job_id].send_json({
                    "type": "progress",
                    "data": {
                        "step": step,
                        "status": "completed",
                        "message": f"{step.replace('_', ' ').title()} complete",
                        "progress": 100
                    }
                })

            # Send final completion message
            await active_connections[job_id].send_json({
                "type": "complete",
                "data": {
                    "videoUrl": video_url,
                    "duration": duration,
                    "topic": topic
                }
            })
        except Exception as e:
            logger.error(f"Failed to send completion: {e}")


async def send_error(job_id: str, error: str):
    """Send error notification via WebSocket."""
    if job_id in job_status:
        job_status[job_id]["status"] = "failed"
        job_status[job_id]["error"] = error

    if job_id in active_connections:
        try:
            await active_connections[job_id].send_json(
                {"type": "error", "message": error}
            )
        except Exception as e:
            logger.error(f"Failed to send error: {e}")


async def generate_video_pipeline(
    job_id: str,
    topic: str,
    duration_seconds: int,
    quality: str,
    enable_subtitles: bool,
    celebrity: str = "drake",
    resume_from_job: Optional[str] = None,
):
    """
    Main video generation pipeline with celebrity lip-sync.

    Args:
        job_id: Unique job identifier
        topic: Educational topic
        duration_seconds: Target duration
        quality: Video quality setting
        enable_subtitles: Whether to add subtitles
        celebrity: Celebrity for lip-synced video (drake, sydney_sweeney) - used as fallback
                   Note: Celebrity is now automatically selected per segment based on speaker:
                   - Male characters (Alex, Jamie, etc.) -> Drake
                   - Female characters (Maya, Emma, etc.) -> Sydney Sweeney
        resume_from_job: Optional job ID to resume from (skips cleanup and completed steps)
    """
    try:
        # Wait for WebSocket connection before starting
        logger.info(f"Waiting for WebSocket connection for job {job_id}")
        max_wait_time = 30  # Wait up to 30 seconds for WebSocket connection
        wait_interval = 0.5
        waited = 0

        while job_id not in active_connections and waited < max_wait_time:
            await asyncio.sleep(wait_interval)
            waited += wait_interval

        if job_id not in active_connections:
            logger.warning(f"WebSocket not connected after {max_wait_time}s, proceeding anyway for job {job_id}")
        else:
            logger.info(f"WebSocket connected, starting video generation for job {job_id}: {topic}")

        # Ensure output directory exists
        BASE_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

        # Handle resume logic
        resume_mode = resume_from_job is not None
        if resume_mode:
            # Resume from existing job directory
            job_dir = BASE_OUTPUT_DIR / resume_from_job
            if not job_dir.exists():
                raise Exception(f"Cannot resume: job directory not found: {resume_from_job}")

            logger.info(f"RESUME MODE: Resuming from job {resume_from_job}")

            # Detect completed steps
            resume_detector = ResumeDetector(job_dir)
            completed_steps = resume_detector.detect_completed_steps()
            resume_point = resume_detector.get_resume_point()

            logger.info(resume_detector.get_summary())
            logger.info(f"Resuming from: {resume_point}")

            # Send resume summary to user
            await send_progress_update(
                job_id,
                f"Resuming from {resume_point} (skipping {sum(completed_steps.values())} completed steps)",
                5
            )
        else:
            # Create new job output directory
            job_dir = BASE_OUTPUT_DIR / job_id
            job_dir.mkdir(parents=True, exist_ok=True)
            completed_steps = {}
            resume_point = "script"

        # Initialize pipeline modules
        script_gen = ScriptGenerator(OPENAI_API_KEY)
        audio_gen = AudioGenerator(OPENAI_API_KEY)
        timestamp_ext = TimestampExtractor(OPENAI_API_KEY)
        visual_gen = VisualScriptGenerator(OPENAI_API_KEY)
        manim_gen = ManimGenerator(OPENAI_API_KEY)

        # Image-to-video with configurable model (defaults to seedance for cost savings)
        img_to_video_model = os.getenv("IMAGE_TO_VIDEO_MODEL")
        img_to_video_gen = ImageToVideoGenerator(REPLICATE_API_TOKEN, model=img_to_video_model)

        # Lip-sync with configurable model (defaults to kling for cost savings)
        lipsync_model = os.getenv("LIPSYNC_MODEL")
        lipsync_gen = LipsyncGenerator(REPLICATE_API_TOKEN, model=lipsync_model)

        video_stitcher = VideoStitcher()

        # Get celebrity image
        celebrity_image = CELEBRITY_IMAGES.get(celebrity, CELEBRITY_IMAGES["drake"])
        if not celebrity_image.exists():
            raise FileNotFoundError(f"Celebrity image not found: {celebrity_image}")

        # Step 1: Generate Script
        script_path = job_dir / "script.json"
        if completed_steps.get("script"):
            logger.info("⏩ Skipping script generation (already completed)")
            script = json.loads(script_path.read_text(encoding="utf-8"))
            await send_progress_update(job_id, "Script loaded from cache", 10)
        else:
            await send_progress_update(job_id, "Generating script...", 5)
            script = await script_gen.generate_script(
                topic=topic,
                duration_seconds=duration_seconds,
                progress_callback=lambda msg, prog: asyncio.create_task(
                    send_progress_update(job_id, msg, prog)
                ),
            )
            logger.info(f"Script generated with {len(script)} segments")

            # Save script
            script_path.write_text(json.dumps(script, indent=2), encoding="utf-8")

        # Step 2: Generate Audio
        audio_dir = job_dir / "audio_segments"
        final_audio_path = job_dir / "narration.mp3"
        if completed_steps.get("audio"):
            logger.info("⏩ Skipping audio generation (already completed)")
            await send_progress_update(job_id, "Audio loaded from cache", 25)
        else:
            await send_progress_update(job_id, "Generating audio...", 20)
            await audio_gen.generate_full_audio(
                script=script,
                output_dir=audio_dir,
                final_output_path=final_audio_path,
                progress_callback=lambda msg, prog: asyncio.create_task(
                    send_progress_update(job_id, msg, prog)
                ),
            )
            logger.info(f"Audio generated: {final_audio_path}")

        # Step 3: Extract Timestamps
        srt_path = job_dir / "subtitles.srt"
        if completed_steps.get("timestamps"):
            logger.info("⏩ Skipping timestamp extraction (already completed)")
            # Load timestamp data from SRT (we'll extract it)
            # For now, we need to regenerate this step's data since we don't save timestamp_data
            await send_progress_update(job_id, "Re-extracting timestamps (needed for alignment)...", 38)
            timestamp_data = await timestamp_ext.extract_timestamps(
                audio_path=final_audio_path,
                output_srt_path=srt_path,
                progress_callback=lambda msg, prog: asyncio.create_task(
                    send_progress_update(job_id, msg, prog)
                ),
            )
            aligned_script = await timestamp_ext.align_script_with_timestamps(
                script=script,
                timestamp_data=timestamp_data,
            )
            await send_progress_update(job_id, "Timestamps loaded from cache", 40)
        else:
            await send_progress_update(job_id, "Extracting timestamps...", 35)
            timestamp_data = await timestamp_ext.extract_timestamps(
                audio_path=final_audio_path,
                output_srt_path=srt_path,
                progress_callback=lambda msg, prog: asyncio.create_task(
                    send_progress_update(job_id, msg, prog)
                ),
            )
            logger.info(f"Timestamps extracted: {len(timestamp_data['segments'])} segments")

            # Align script with timestamps
            aligned_script = await timestamp_ext.align_script_with_timestamps(
                script=script,
                timestamp_data=timestamp_data,
                progress_callback=lambda msg, prog: asyncio.create_task(
                    send_progress_update(job_id, msg, prog)
                ),
            )

        # Step 4: Generate Visual Instructions
        visual_path = job_dir / "visual_instructions.json"
        if completed_steps.get("visual_instructions"):
            logger.info("⏩ Skipping visual instructions (already completed)")
            visual_instructions = json.loads(visual_path.read_text(encoding="utf-8"))
            await send_progress_update(job_id, "Visual instructions loaded from cache", 47)
        else:
            await send_progress_update(job_id, "Generating visual instructions...", 45)
            visual_instructions = await visual_gen.generate_visual_instructions(
                script=script,
                topic=topic,
                aligned_timestamps=aligned_script,
                progress_callback=lambda msg, prog: asyncio.create_task(
                    send_progress_update(job_id, msg, prog)
                ),
            )
            logger.info(f"Visual instructions generated: {len(visual_instructions)} segments")

            # Save visual instructions
            visual_path.write_text(
                json.dumps(visual_instructions, indent=2), encoding="utf-8"
            )

        # Step 5: Generate Manim Code
        manim_file = job_dir / "animation.py"
        audio_duration = timestamp_data.get('duration', 60.0)
        if completed_steps.get("manim_code"):
            logger.info("⏩ Skipping Manim code generation (already completed)")
            await send_progress_update(job_id, "Manim code loaded from cache", 50)
        else:
            await send_progress_update(job_id, "Generating Manim code...", 50)
            await manim_gen.generate_manim_code(
                visual_instructions=visual_instructions,
                topic=topic,
                output_path=manim_file,
                target_duration=audio_duration,
                progress_callback=lambda msg, prog: asyncio.create_task(
                    send_progress_update(job_id, msg, prog)
                ),
            )
            logger.info(f"Manim code generated: {manim_file}")

        # Step 6: Render Manim Video (TOP HALF - 9:16 format)
        manim_output_dir = job_dir / "manim_output"
        manim_video = None

        if completed_steps.get("manim_render"):
            logger.info("⏩ Skipping Manim render (already completed)")
            # Find existing rendered video
            manim_videos = list(manim_output_dir.rglob("manim_output.mp4"))
            if manim_videos:
                manim_video = manim_videos[0]
                logger.info(f"Using existing Manim video: {manim_video}")
                await send_progress_update(job_id, "Manim video loaded from cache", 58)
            else:
                logger.warning("Manim render marked complete but video not found, re-rendering...")
                completed_steps["manim_render"] = False

        # Wrap in retry loop in case full render fails with errors test render missed
        if not completed_steps.get("manim_render"):
            for render_attempt in range(3):  # Up to 3 render attempts
                try:
                    await send_progress_update(job_id, f"Rendering Manim animation (attempt {render_attempt + 1}/3)...", 55)
                    manim_video = await manim_gen.render_manim_video(
                        manim_file=manim_file,
                        output_dir=manim_output_dir,
                        quality=quality,
                        aspect_ratio="9:8",  # 9:8 aspect ratio for top half (will be stacked with 9:8 bottom to make 9:16)
                        progress_callback=lambda msg, prog: asyncio.create_task(
                            send_progress_update(job_id, msg, prog)
                        ),
                    )
                    logger.info(f"Manim video rendered: {manim_video}")
                    break  # Success! Exit retry loop
                except Exception as render_error:
                    logger.warning(f"Manim render attempt {render_attempt + 1} failed: {render_error}")
                    if render_attempt < 2:  # Not last attempt
                        # Regenerate code with the render error
                        await send_progress_update(job_id, f"Render failed, regenerating code...", 52)
                        manim_file = await manim_gen.generate_manim_code(
                            visual_instructions=visual_instructions,
                            topic=topic,
                            output_path=manim_file,
                            target_duration=audio_duration,
                            progress_callback=lambda msg, prog: asyncio.create_task(
                                send_progress_update(job_id, msg, prog)
                            ),
                        )
                        logger.info(f"Manim code regenerated after render failure")
                    else:
                        # Last attempt failed, raise error
                        raise

        if not manim_video:
            raise Exception("Failed to render Manim video after 3 attempts")

        # Step 7: Generate Celebrity Videos from Image (BOTTOM HALF) - PER SEGMENT
        # Get individual audio segment files
        audio_segment_files = sorted(list(audio_dir.glob("segment_*.mp3")))
        logger.info(f"Found {len(audio_segment_files)} audio segments to process")

        if not audio_segment_files:
            raise Exception("No audio segments found! Cannot generate celebrity videos.")

        celebrity_video_dir = job_dir / "celebrity_videos"
        celebrity_video_dir.mkdir(parents=True, exist_ok=True)
        total_segments = len(audio_segment_files)

        if completed_steps.get("celebrity_videos"):
            logger.info("⏩ Skipping celebrity video generation (already completed)")
            # Load existing trimmed celebrity videos (prefer trimmed versions)
            celebrity_video_segments = sorted(list(celebrity_video_dir.glob("segment_*_trimmed.mp4")))
            if not celebrity_video_segments:
                # Fallback to non-trimmed if trimmed don't exist (backwards compatibility)
                celebrity_video_segments = sorted(list(celebrity_video_dir.glob("segment_*.mp4")))
            logger.info(f"Using {len(celebrity_video_segments)} existing celebrity videos")
            await send_progress_update(job_id, "Celebrity videos loaded from cache", 70)
        else:
            await send_progress_update(job_id, "Generating celebrity videos per segment...", 60)

            # Generate celebrity video for each segment
            celebrity_video_segments = []

            for idx, audio_segment_path in enumerate(audio_segment_files):
                segment_progress = int(60 + (idx / total_segments) * 15)
                await send_progress_update(
                    job_id,
                    f"Generating celebrity video {idx + 1}/{total_segments}...",
                    segment_progress
                )

                # Get duration of this audio segment using ffprobe
                segment_duration = await video_stitcher._get_duration(audio_segment_path)
                logger.info(f"Segment {idx}: {segment_duration:.2f}s")

                # Generate celebrity video for this segment
                celebrity_segment_path = celebrity_video_dir / f"segment_{idx:03d}.mp4"

                # Extract speaker from audio filename (format: segment_XXX_SpeakerName.mp3)
                # Map speaker to celebrity image based on voice type:
                # - Male voices (onyx, echo, fable) -> Drake
                # - Female voices (nova, shimmer, alloy) -> Sydney Sweeney
                speaker = audio_segment_path.stem.split('_')[-1]  # Get speaker name from filename

                # Get the voice assigned to this speaker from the script
                # The script uses the first speaker as "alloy" (typically assigned to Alex)
                # and alternates voices for subsequent speakers
                # For now, we'll use character names to determine celebrity
                # Common patterns: Alex/Jamie (boy names) -> Drake, Maya/Emma (girl names) -> Sydney

                # Define male and female character names (expand as needed)
                male_names = ["alex", "jamie", "john", "mike", "tom", "sam"]
                female_names = ["maya", "emma", "sarah", "lisa", "anna", "jane"]

                speaker_lower = speaker.lower()

                # Map speaker to celebrity based on name
                if speaker_lower in male_names or speaker_lower.endswith("alex"):
                    segment_celebrity_image = CELEBRITY_IMAGES["drake"]
                    logger.info(f"Segment {idx}: Using Drake for {speaker}")
                elif speaker_lower in female_names or speaker_lower.endswith("maya"):
                    segment_celebrity_image = CELEBRITY_IMAGES["sydney_sweeney"]
                    logger.info(f"Segment {idx}: Using Sydney Sweeney for {speaker}")
                else:
                    # Fallback: alternate based on segment index (even = Drake, odd = Sydney)
                    # This works well for 2-person conversations
                    if idx % 2 == 0:
                        segment_celebrity_image = CELEBRITY_IMAGES["drake"]
                        logger.info(f"Segment {idx}: Using Drake for unknown speaker '{speaker}' (even index)")
                    else:
                        segment_celebrity_image = CELEBRITY_IMAGES["sydney_sweeney"]
                        logger.info(f"Segment {idx}: Using Sydney Sweeney for unknown speaker '{speaker}' (odd index)")

                # Use varied prompts for natural variety
                prompts = [
                    "natural talking expression, engaging eye contact, friendly smile, slight head nod",
                    "expressive speaking, warm expression, subtle gestures, natural movement",
                    "animated talking, enthusiastic expression, gentle head tilt, engaging presence",
                    "conversational speaking, genuine smile, soft eye contact, relaxed posture",
                    "energetic narration, dynamic expression, natural hand gestures, confident delivery",
                ]
                prompt = prompts[idx % len(prompts)]

                await img_to_video_gen.generate_video_from_image(
                    image_path=segment_celebrity_image,
                    duration=segment_duration,
                    prompt=prompt,
                    output_path=celebrity_segment_path,
                    aspect_ratio="9:16",
                )

                # CRITICAL: Trim video to EXACT audio duration to prevent sync drift
                # The image-to-video models (Seedance/Kling) don't generate exact durations
                trimmed_segment_path = celebrity_video_dir / f"segment_{idx:03d}_trimmed.mp4"
                logger.info(f"Trimming celebrity video {idx} to exact duration: {segment_duration:.2f}s")

                await video_stitcher.trim_video_to_duration(
                    video_path=celebrity_segment_path,
                    duration=segment_duration,
                    output_path=trimmed_segment_path,
                )

                # Verify trimmed video duration
                trimmed_duration = await video_stitcher._get_duration(trimmed_segment_path)
                logger.info(f"Trimmed video {idx} duration: {trimmed_duration:.2f}s (target: {segment_duration:.2f}s)")

                celebrity_video_segments.append(trimmed_segment_path)
                logger.info(f"Celebrity video segment {idx} trimmed and ready: {trimmed_segment_path}")

        # Step 8: Lip-sync Each Celebrity Video Segment with Its Audio
        lipsynced_video_dir = job_dir / "lipsynced_videos"
        lipsynced_video_dir.mkdir(parents=True, exist_ok=True)

        if completed_steps.get("lipsynced_videos"):
            logger.info("⏩ Skipping lip-sync (already completed)")
            # Load existing lip-synced videos
            lipsynced_segments = sorted(list(lipsynced_video_dir.glob("lipsynced_*.mp4")))
            logger.info(f"Using {len(lipsynced_segments)} existing lip-synced videos")
            await send_progress_update(job_id, "Lip-synced videos loaded from cache", 85)
        else:
            await send_progress_update(job_id, "Lip-syncing video segments with audio...", 75)

            lipsynced_segments = []

            for idx, (video_segment, audio_segment) in enumerate(zip(celebrity_video_segments, audio_segment_files)):
                segment_progress = int(75 + (idx / total_segments) * 15)
                await send_progress_update(
                    job_id,
                    f"Lip-syncing segment {idx + 1}/{total_segments}...",
                    segment_progress
                )

                # Lip-sync this segment
                lipsynced_segment_path = lipsynced_video_dir / f"lipsynced_{idx:03d}.mp4"

                await lipsync_gen.sync_audio_to_video(
                    video_path=video_segment,
                    audio_path=audio_segment,
                    output_path=lipsynced_segment_path,
                )

                # Verify lip-synced video duration matches audio
                lipsynced_duration = await video_stitcher._get_duration(lipsynced_segment_path)
                audio_duration_check = await video_stitcher._get_duration(audio_segment)
                logger.info(f"Lip-synced segment {idx} duration: {lipsynced_duration:.2f}s (audio: {audio_duration_check:.2f}s)")

                # If lip-sync changed duration, trim to match audio exactly
                if abs(lipsynced_duration - audio_duration_check) > 0.1:  # More than 100ms difference
                    logger.warning(f"Lip-sync changed duration by {abs(lipsynced_duration - audio_duration_check):.2f}s, trimming to match audio")
                    trimmed_lipsynced_path = lipsynced_video_dir / f"lipsynced_{idx:03d}_trimmed.mp4"
                    await video_stitcher.trim_video_to_duration(
                        video_path=lipsynced_segment_path,
                        duration=audio_duration_check,
                        output_path=trimmed_lipsynced_path,
                    )
                    lipsynced_segments.append(trimmed_lipsynced_path)
                    logger.info(f"Lip-synced segment {idx} trimmed to exact audio duration: {trimmed_lipsynced_path}")
                else:
                    lipsynced_segments.append(lipsynced_segment_path)
                    logger.info(f"Lip-synced segment {idx} duration matches audio: {lipsynced_segment_path}")

            # Step 8b: Concatenate all lip-synced segments into one video
            await send_progress_update(job_id, "Concatenating lip-synced segments...", 88)

            lipsynced_video = job_dir / "celebrity_lipsynced_full.mp4"

            await video_stitcher.concatenate_videos(
                video_paths=lipsynced_segments,
                output_path=lipsynced_video,
                progress_callback=lambda msg, prog: asyncio.create_task(
                    send_progress_update(job_id, msg, int(88 + prog * 0.02))
                ),
            )
            logger.info(f"All lip-synced segments concatenated: {lipsynced_video}")

        # Load concatenated video if it exists (for resume case)
        lipsynced_video = job_dir / "celebrity_lipsynced_full.mp4"

        # Step 9: Composite Top (Manim) and Bottom (Celebrity) into 9:16 Video
        composite_video = job_dir / "composite_video.mp4"

        if completed_steps.get("composite"):
            logger.info("⏩ Skipping composite (already completed)")
            await send_progress_update(job_id, "Composite video loaded from cache", 92)
        else:
            await send_progress_update(job_id, "Compositing educational and celebrity videos...", 90)

            await video_stitcher.composite_top_bottom_videos(
                top_video_path=manim_video,
                bottom_video_path=lipsynced_video,
                audio_path=final_audio_path,
                output_path=composite_video,
                progress_callback=lambda msg, prog: asyncio.create_task(
                    send_progress_update(job_id, msg, int(90 + prog * 0.05))
                ),
            )
            logger.info(f"Composite video created: {composite_video}")

        # Step 10: Add Subtitles (if enabled)
        final_video = job_dir / "final_video.mp4"

        if completed_steps.get("final"):
            logger.info("⏩ Final video already exists")
            await send_progress_update(job_id, "Final video loaded from cache", 98)
        elif enable_subtitles:
            await send_progress_update(job_id, "Adding subtitles...", 95)
            await video_stitcher.add_subtitles(
                video_path=composite_video,
                srt_path=srt_path,
                output_path=final_video,
                progress_callback=lambda msg, prog: asyncio.create_task(
                    send_progress_update(job_id, msg, int(95 + prog * 0.05))
                ),
            )
        else:
            # No subtitles, just use composite as final
            final_video = composite_video

        logger.info(f"Final video ready: {final_video}")

        # Send completion
        video_url = f"/api/videos/{job_id}/{final_video.name}"
        video_duration = timestamp_data.get('duration', 0) if 'timestamp_data' in locals() else 0
        await send_completion(job_id, video_url, topic=topic, duration=video_duration)

        logger.info(f"Video generation complete for job {job_id}")

    except Exception as e:
        logger.error(f"Video generation failed for job {job_id}: {e}", exc_info=True)
        await send_error(job_id, str(e))


# Startup event
@app.on_event("startup")
async def startup_event():
    """Startup event - no cleanup here (cleanup happens when user clicks generate)."""
    logger.info("Server started - output folders preserved for resume functionality")


# API Endpoints
@app.get("/")
async def root():
    """Root endpoint."""
    return {
        "name": "Educational Video Generation API",
        "version": "1.0.0",
        "status": "running",
    }


@app.get("/health")
async def health_check():
    """Health check endpoint."""
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}


@app.post("/api/generate", response_model=VideoGenerationResponse)
async def generate_video(
    request: VideoGenerationRequest, background_tasks: BackgroundTasks
):
    """
    Generate an educational video.

    Args:
        request: Video generation parameters

    Returns:
        Job information including job_id for tracking progress
    """
    try:
        # Validate API key
        if not OPENAI_API_KEY:
            raise HTTPException(
                status_code=500, detail="OpenAI API key not configured"
            )

        # Clean up old outputs before starting new job (skip if resuming)
        if request.resume_job_id:
            job_id = request.resume_job_id
            logger.info(f"Resuming job: {job_id}")
        else:
            # Clean up before creating new job
            logger.info("Cleaning up old outputs before starting new job...")
            # cleanup_old_outputs(skip_cleanup=False)

            job_id = str(uuid.uuid4())
            logger.info(f"Created new job: {job_id}")

        # Initialize job status
        job_status[job_id] = {
            "job_id": job_id,
            "status": "waiting_for_connection",
            "progress": 0,
            "message": "Waiting for WebSocket connection..." if not request.resume_job_id else "Resuming from previous job...",
            "topic": request.topic,
            "created_at": datetime.now().isoformat(),
        }

        # Start background task
        background_tasks.add_task(
            generate_video_pipeline,
            job_id=job_id,
            topic=request.topic,
            duration_seconds=request.duration_seconds,
            quality=request.quality,
            enable_subtitles=request.enable_subtitles,
            celebrity=request.celebrity,
            resume_from_job=request.resume_job_id,
        )

        logger.info(f"Video generation job created: {job_id}")

        return VideoGenerationResponse(
            job_id=job_id,
            status="waiting_for_connection",
            message="Video generation job created. Connect to WebSocket to start processing.",
        )

    except Exception as e:
        logger.error(f"Failed to create video generation job: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/jobs/{job_id}", response_model=JobStatusResponse)
async def get_job_status(job_id: str):
    """
    Get status of a video generation job.

    Args:
        job_id: Job identifier

    Returns:
        Current job status
    """
    if job_id not in job_status:
        raise HTTPException(status_code=404, detail="Job not found")

    status = job_status[job_id]
    return JobStatusResponse(
        job_id=status["job_id"],
        status=status["status"],
        progress=status.get("progress", 0),
        message=status.get("message", ""),
        video_url=status.get("video_url"),
        error=status.get("error"),
    )


@app.get("/api/videos/{job_id}/{filename}")
async def get_video(job_id: str, filename: str):
    """
    Download generated video.

    Args:
        job_id: Job identifier
        filename: Video filename

    Returns:
        Video file
    """
    video_path = BASE_OUTPUT_DIR / job_id / filename

    if not video_path.exists():
        raise HTTPException(status_code=404, detail="Video not found")

    return FileResponse(
        path=video_path,
        media_type="video/mp4",
        filename=f"{job_id}_{filename}",
    )


@app.websocket("/ws/{job_id}")
async def websocket_endpoint(websocket: WebSocket, job_id: str):
    """
    WebSocket endpoint for real-time progress updates.

    Args:
        websocket: WebSocket connection
        job_id: Job identifier to track
    """
    await websocket.accept()
    active_connections[job_id] = websocket

    try:
        logger.info(f"WebSocket connected for job {job_id}")

        # Send current status if job exists
        if job_id in job_status:
            status = job_status[job_id]
            await websocket.send_json(
                {
                    "type": "status",
                    "status": status["status"],
                    "progress": status.get("progress", 0),
                    "message": status.get("message", ""),
                }
            )

        # Keep connection alive with periodic pings
        last_ping = asyncio.get_event_loop().time()
        while True:
            try:
                # Wait for messages with timeout
                await asyncio.wait_for(websocket.receive_text(), timeout=1.0)
            except asyncio.TimeoutError:
                # Send ping every 30 seconds to keep connection alive through Cloudflare
                current_time = asyncio.get_event_loop().time()
                if current_time - last_ping > 30:
                    try:
                        await websocket.send_json({"type": "ping"})
                        last_ping = current_time
                    except Exception as e:
                        logger.warning(f"Failed to send ping: {e}")
                        break
                continue
            except WebSocketDisconnect:
                break

    except WebSocketDisconnect:
        logger.info(f"WebSocket disconnected for job {job_id}")
    except Exception as e:
        logger.error(f"WebSocket error for job {job_id}: {e}")
    finally:
        if job_id in active_connections:
            del active_connections[job_id]


# Run application
if __name__ == "__main__":
    import sys

    port = int(sys.argv[1]) if len(sys.argv) > 1 else 8000

    logger.info(f"Starting Educational Video Generation API on port {port}")

    import os
    is_production = os.getenv("ENVIRONMENT") == "production"

    # Configure reload to ignore all directories where we write files during video generation
    # This prevents WebSocket disconnections when files are created
    if not is_production:
        logger.info("Development mode: Auto-reload DISABLED to prevent WebSocket disconnections during video generation")
        logger.info("Please manually restart the server when you make code changes")
        reload_enabled = False
    else:
        reload_enabled = False

    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=port,
        reload=reload_enabled,
        log_level="info",
    )
